# Awesome Transformer Quantization Paper Collection


- [2024](#2024)
  - [AAAI](#aaai-2024)

- [2023](#2023)
  - [NeurIPS](#neurips-2023)
  - [CVPR](#cvpr-2023)
  - [ICCV](#iccv-2023)
  - [ICML](#icml-2023)
  - [ACL](#acl-2023)

- [2022](#2022)
  - [NeurIPS](#neurips-2022)
  - [ECCV](#eccv-2022)
  - [IJCAI](#ijcai-2022)

- [2021](#2021)
  - [NeurIPS](#neurips-2021)

- [2020](#2020)
  - [ACL](#acl-2020)



# 2024


## AAAI-2024


- Bi-ViT: Pushing the Limit of Vision Transformer Quantization [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28109)] [[arxiv](https://arxiv.org/abs/2305.12354)] [[paper with code](https://paperswithcode.com/paper/bi-vit-pushing-the-limit-of-vision)]

- AQ-DETR: Low-Bit Quantized Detection Transformer with Auxiliary Queries [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29487)]



# 2023


## NeurIPS-2023


- Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1abc87c67cc400a67b869358e627fe37-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2310.14228)] [[paper with code](https://paperswithcode.com/paper/hierarchical-vector-quantized-transformer-for-1)] [[code](https://github.com/ruiyinglu/hvq-trans)] [[openview](https://openreview.net/forum?id=clJTNssgn6)]

- PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1c92edb990a05f2269f0cc3afbb4c952-Abstract-Conference.html)]

- Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/edbcb7583fd8921dad78adecfe06a99b-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2306.12929)] [[paper with code](https://paperswithcode.com/paper/quantizable-transformers-removing-outliers-by)] [[code](https://github.com/qualcomm-ai-research/outlier-free-transformers)] [[openview](https://openreview.net/forum?id=sbusw6LD41)]


## CVPR-2023


- Boost Vision Transformer With GPU-Friendly Sparsity and Quantization [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Boost_Vision_Transformer_With_GPU-Friendly_Sparsity_and_Quantization_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2305.10727)] [[paper with code](https://paperswithcode.com/paper/boost-vision-transformer-with-gpu-friendly-1)]

- NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_NoisyQuant_Noisy_Bias-Enhanced_Post-Training_Activation_Quantization_for_Vision_Transformers_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2211.16056)] [[paper with code](https://paperswithcode.com/paper/noisyquant-noisy-bias-enhanced-post-training)]

- Q-DETR: An Efficient Low-Bit Quantized Detection Transformer [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2304.00253)] [[paper with code](https://paperswithcode.com/paper/q-detr-an-efficient-low-bit-quantized)] [[code](https://github.com/stevetsui/q-detr)]


## ICCV-2023


- Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Frumkin_Jumping_through_Local_Minima_Quantization_in_the_Loss_Landscape_of_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.10814)] [[paper with code](https://paperswithcode.com/paper/jumping-through-local-minima-quantization-in)] [[code](https://github.com/enyac-group/evol-q)]

- Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Corona-Figueroa_Unaligned_2D_to_3D_Translation_with_Conditional_Vector-Quantized_Code_Diffusion_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.14152)] [[paper with code](https://paperswithcode.com/paper/unaligned-2d-to-3d-translation-with)] [[code](https://github.com/samb-t/x2ct-vqvae)]

- Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Su_Name_Your_Colour_For_the_Task_Artificially_Discover_Colour_Naming_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2212.03434)] [[paper with code](https://paperswithcode.com/paper/name-your-colour-for-the-task-artificially)] [[code](https://github.com/ryeocthiv/cqformer)]

- I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2207.01405)] [[paper with code](https://paperswithcode.com/paper/i-vit-integer-only-quantization-for-efficient)] [[code](https://github.com/zkkli/i-vit)]

- RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2212.08254)] [[paper with code](https://paperswithcode.com/paper/repq-vit-scale-reparameterization-for-post)] [[code](https://github.com/zkkli/repq-vit)]


## ICML-2023


- SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process [[paper](https://proceedings.mlr.press/v202/li23aj.html)] [[arxiv](https://arxiv.org/abs/2310.16336)] [[paper with code](https://paperswithcode.com/paper/smurf-thp-score-matching-based-uncertainty)] [[code](https://github.com/zichongli5/smurf-thp)]

- Oscillation-free Quantization for Low-bit Vision Transformers [[paper](https://proceedings.mlr.press/v202/liu23w.html)] [[arxiv](https://arxiv.org/abs/2302.02210)] [[paper with code](https://paperswithcode.com/paper/oscillation-free-quantization-for-low-bit)] [[code](https://github.com/nbasyl/OFQ)]


## ACL-2023


- Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models [[paper](https://aclanthology.org/2023.acl-short.114/)] [[arxiv](https://arxiv.org/abs/2307.05972)] [[paper with code](https://paperswithcode.com/paper/self-distilled-quantization-achieving-high)]


# 2022


## NeurIPS-2022


- ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2206.01861)] [[paper with code](https://paperswithcode.com/paper/zeroquant-efficient-and-affordable-post)] [[code](https://github.com/microsoft/DeepSpeed)]

- Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/deb921bff461a7b0a5c344a4871e7101-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2210.06707)] [[paper with code](https://paperswithcode.com/paper/q-vit-accurate-and-fully-quantized-low-bit)] [[code](https://github.com/yanjingli0202/q-vit)]


## ECCV-2022


- Patch Similarity Aware Data-Free Quantization for Vision Transformers [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1580_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2203.02250)] [[paper with code](https://paperswithcode.com/paper/patch-similarity-aware-data-free-quantization)] [[code](https://github.com/zkkli/psaq-vit)]

- PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7856_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2111.12293)]

- Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5081_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2111.12701)] [[paper with code](https://paperswithcode.com/paper/unleashing-transformers-parallel-token)] [[code](https://github.com/samb-t/unleashing-transformers)]


## IJCAI-2022


- FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer [[paper](https://www.ijcai.org/proceedings/2022/164)] [[arxiv](https://arxiv.org/abs/2111.13824)] [[paper with code](https://paperswithcode.com/paper/fq-vit-fully-quantized-vision-transformer)] [[code](https://github.com/megvii-research/FQ-ViT)]


# 2021


## NeurIPS-2021


- Post-Training Quantization for Vision Transformer [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html)] [[arxiv](https://arxiv.org/abs/2401.14895)] [[paper with code](https://paperswithcode.com/paper/post-training-quantization-for-vision)]


# 2020


## ACL-2020


- Quantifying Attention Flow in Transformers [[paper](https://aclanthology.org/2020.acl-main.385/)] [[arxiv](https://arxiv.org/abs/2005.00928)] [[paper with code](https://paperswithcode.com/paper/quantifying-attention-flow-in-transformers)] [[code](https://github.com/samiraabnar/attention_flow)]