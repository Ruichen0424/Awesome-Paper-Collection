# Awesome Autonomous Driving Paper Collection

- [2024](#2024)
  - [CVPR](#cvpr-2024)
  - [ICLR](#iclr-2024)
  - [AAAI](#aaai-2024)

- [2023](#2023)
  - [NeurIPS](#neurips-2023)
  - [CVPR](#cvpr-2023)
  - [ICLR](#iclr-2023)
  - [ICCV](#iccv-2023)
  - [ICML](#icml-2023)
  - [IJCAI](#ijcai-2023)
  - [AAAI](#aaai-2023)

- [2022](#2022)
  - [NeurIPS](#neurips-2022)
  - [CVPR](#cvpr-2022)
  - [ECCV](#eccv-2022)
  - [AAAI](#aaai-2022)

- [2021](#2021)
  - [CVPR](#cvpr-2021)
  - [ICCV](#iccv-2021)
  - [ICML](#icml-2021)
  - [AAAI](#aaai-2021)

- [2020](#2020)
  - [CVPR](#cvpr-2020)
  - [ICLR](#iclr-2020)
  - [ECCV](#eccv-2020)
  - [AAAI](#aaai-2020)

- [2019](#2019)
  - [CVPR](#cvpr-2019)
  - [ICCV](#iccv-2019)
  - [IJCAI](#ijcai-2019)

- [2018](#2018)
  - [ECCV](#eccv-2018)



# 2024


## CVPR-2024


- Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Ding_Holistic_Autonomous_Driving_Understanding_by_Birds-Eye-View_Injected_Multi-Modal_Large_Models_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2401.00988)] [[paper with code](https://paperswithcode.com/paper/holistic-autonomous-driving-understanding-by)] [[code](https://github.com/xmed-lab/nuinstruct)]

- Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Physical_3D_Adversarial_Attacks_against_Monocular_Depth_Estimation_in_Autonomous_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2403.17301)] [[paper with code](https://paperswithcode.com/paper/physical-3d-adversarial-attacks-against)] [[code](https://github.com/gandolfczjh/3d2fool)]

- Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Cam4DOcc_Benchmark_for_Camera-Only_4D_Occupancy_Forecasting_in_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2311.17663)] [[paper with code](https://paperswithcode.com/paper/cam4docc-benchmark-for-camera-only-4d)] [[code](https://github.com/haomo-ai/cam4docc)]

- VLP: Vision Language Planning for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Pan_VLP_Vision_Language_Planning_for_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2401.05577)] [[paper with code](https://paperswithcode.com/paper/vlp-vision-language-planning-for-autonomous)]

- Bootstrapping Autonomous Driving Radars with Self-Supervised Learning [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Hao_Bootstrapping_Autonomous_Driving_Radars_with_Self-Supervised_Learning_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2312.04519)] [[paper with code](https://paperswithcode.com/paper/bootstrapping-autonomous-radars-with-self)]

- Generalized Predictive Model for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Generalized_Predictive_Model_for_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2403.09630)] [[paper with code](https://paperswithcode.com/paper/generalized-predictive-model-for-autonomous)] [[code](https://github.com/opendrivelab/driveagi)]

- Visual Point Cloud Forecasting enables Scalable Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2312.17655)] [[paper with code](https://paperswithcode.com/paper/visual-point-cloud-forecasting-enables)] [[code](https://github.com/opendrivelab/vidar)]

- Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Pourkeshavarz_Adversarial_Backdoor_Attack_by_Naturalistic_Data_Poisoning_on_Trajectory_Prediction_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2306.15755)] [[paper with code](https://paperswithcode.com/paper/imposition-implicit-backdoor-attack-through)]

- On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Feng_On_the_Road_to_Portability_Compressing_End-to-End_Motion_Planner_for_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2403.01238)] [[paper with code](https://paperswithcode.com/paper/on-the-road-to-portability-compressing-end-to)] [[code](https://github.com/tulerfeng/PlanKD)]

- Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving? [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Is_Ego_Status_All_You_Need_for_Open-Loop_End-to-End_Autonomous_CVPR_2024_paper.html)] [[paper with code](https://paperswithcode.com/paper/is-ego-status-all-you-need-for-open-loop-end)] [[code](https://github.com/nvlabs/bev-planner)]

- Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Driving_into_the_Future_Multiview_Visual_Forecasting_and_Planning_with_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2311.17918)] [[paper with code](https://paperswithcode.com/paper/driving-into-the-future-multiview-visual)] [[code](https://github.com/bravegroup/drive-wm)]

- DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2405.04390)] [[paper with code](https://paperswithcode.com/paper/driveworld-4d-pre-trained-scene-understanding)]

- DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_DrivingGaussian_Composite_Gaussian_Splatting_for_Surrounding_Dynamic_Autonomous_Driving_Scenes_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2312.07920)] [[paper with code](https://paperswithcode.com/paper/drivinggaussian-composite-gaussian-splatting)] [[code](https://github.com/vdigpku/drivinggaussian)]

- UniPAD: A Universal Pre-training Paradigm for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_UniPAD_A_Universal_Pre-training_Paradigm_for_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2310.08370)] [[paper with code](https://paperswithcode.com/paper/unipad-a-universal-pre-training-paradigm-for)] [[code](https://github.com/Nightmare-n/UniPAD)]

- NeuRAD: Neural Rendering for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Tonderski_NeuRAD_Neural_Rendering_for_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2311.15260)] [[paper with code](https://paperswithcode.com/paper/neurad-neural-rendering-for-autonomous)] [[code](https://github.com/georghess/neurad-studio)]

- Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Light_the_Night_A_Multi-Condition_Diffusion_Framework_for_Unpaired_Low-Light_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2404.04804)] [[paper with code](https://paperswithcode.com/paper/light-the-night-a-multi-condition-diffusion)]

- Panacea: Panoramic and Controllable Video Generation for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wen_Panacea_Panoramic_and_Controllable_Video_Generation_for_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2311.16813)] [[paper with code](https://paperswithcode.com/paper/panacea-panoramic-and-controllable-video)]

- LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Ma_LaMPilot_An_Open_Benchmark_Dataset_for_Autonomous_Driving_with_Language_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2312.04372)] [[paper with code](https://paperswithcode.com/paper/lampilot-an-open-benchmark-dataset-for)] [[code](https://github.com/purduedigitaltwin/lampilot)]

- SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Xie_SynFog_A_Photo-realistic_Synthetic_Fog_Dataset_based_on_End-to-end_Imaging_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2403.17094)] [[paper with code](https://paperswithcode.com/paper/synfog-a-photo-realistic-synthetic-fog)]

- PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Weng_PARA-Drive_Parallelized_Architecture_for_Real-time_Autonomous_Driving_CVPR_2024_paper.html)] [[paper with code](https://paperswithcode.com/paper/para-drive-parallelized-architecture-for-real)]

- Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wei_Editable_Scene_Simulation_for_Autonomous_Driving_via_Collaborative_LLM-Agents_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2402.05746)] [[paper with code](https://paperswithcode.com/paper/editable-scene-simulation-for-autonomous)] [[code](https://github.com/yifanlu0227/chatsim)]

- AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_AIDE_An_Automatic_Data_Engine_for_Object_Detection_in_Autonomous_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2403.17373)] [[paper with code](https://paperswithcode.com/paper/aide-an-automatic-data-engine-for-object)]

- Feedback-Guided Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Feedback-Guided_Autonomous_Driving_CVPR_2024_paper.html)] [[paper with code](https://paperswithcode.com/paper/feedback-guided-autonomous-driving)]

- Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Adaptive_Fusion_of_Single-View_and_Multi-View_Depth_for_Autonomous_Driving_CVPR_2024_paper.html)] [[arxiv](https://arxiv.org/abs/2403.07535)] [[paper with code](https://paperswithcode.com/paper/adaptive-fusion-of-single-view-and-multi-view)] [[code](https://github.com/junda24/afnet)]

- CaDeT: a Causal Disentanglement Approach for Robust Trajectory Prediction in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Pourkeshavarz_CaDeT_a_Causal_Disentanglement_Approach_for_Robust_Trajectory_Prediction_in_CVPR_2024_paper.html)] [[paper with code](https://paperswithcode.com/paper/cadet-a-causal-disentanglement-approach-for)]


## ICLR-2024


- Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion [[paper](https://iclr.cc/virtual/2024/poster/18691)] [[arxiv](https://arxiv.org/abs/2311.01017)] [[paper with code](https://paperswithcode.com/paper/learning-unsupervised-world-models-for)] [[openreview](https://openreview.net/forum?id=Psl75UCoZM)]

- DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models [[paper](https://iclr.cc/virtual/2024/poster/18729)] [[arxiv](https://arxiv.org/abs/2309.16292)] [[paper with code](https://paperswithcode.com/paper/dilu-a-knowledge-driven-approach-to)] [[code](https://github.com/PJLab-ADG/DiLu)] [[openreview](https://openreview.net/forum?id=OqTMUPuLuC)]

- LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving [[paper](https://iclr.cc/virtual/2024/poster/18846)] [[arxiv](https://arxiv.org/abs/2312.16108)] [[paper with code](https://paperswithcode.com/paper/lanesegnet-map-learning-with-lane-segment)] [[code](https://github.com/OpenDriveLab/LaneSegNet)] [[openreview](https://openreview.net/forum?id=LsURkIPYR5)]

- ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation [[paper](https://iclr.cc/virtual/2024/poster/19579)] [[arxiv](https://arxiv.org/abs/2309.05527)] [[paper with code](https://paperswithcode.com/paper/resimad-zero-shot-3d-domain-transfer-for)] [[code](https://github.com/pjlab-adg/3dtrans)] [[openreview](https://openreview.net/forum?id=1d2cLKeNgY)]

- UC-NERF: Neural Radiance Field for Under-Calibrated Multi-View Cameras in Autonomous Driving [[paper](https://iclr.cc/virtual/2024/poster/18336)] [[arxiv](https://arxiv.org/abs/2311.16945)] [[paper with code](https://paperswithcode.com/paper/uc-nerf-neural-radiance-field-for-under)] [[openreview](https://openreview.net/forum?id=bLKcCe7hYh)]


## AAAI-2024


- M-BEV: Masked BEV Perception for Robust Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/27880)] [[arxiv](https://arxiv.org/abs/2312.12144)] [[paper with code](https://paperswithcode.com/paper/m-bev-masked-bev-perception-for-robust)] [[code](https://github.com/sranc3/m-bev)]

- SDAC: A Multimodal Synthetic Dataset for Anomaly and Corner Case Detection in Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/27961)]

- DALDet: Depth-Aware Learning Based Object Detection for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/27996)]

- MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28027)] [[arxiv](https://arxiv.org/abs/2312.06988)] [[paper with code](https://paperswithcode.com/paper/mwsis-multimodal-weakly-supervised-instance)] [[code](https://github.com/jiangxb98/mwsis-plugin)]

- VLM2Scene: Self-Supervised Image-Text-LiDAR Learning with Foundation Models for Autonomous Driving Scene Understanding [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28121)]

- BEV-MAE: Bird's Eye View Masked Autoencoders for Point Cloud Pre-training in Autonomous Driving Scenarios [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28141)] [[arxiv](https://arxiv.org/abs/2212.05758)]

- SlowTrack: Increasing the Latency of Camera-Based Perception in Autonomous Driving Using Adversarial Examples [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28200)] [[arxiv](https://arxiv.org/abs/2312.09520)] [[paper with code](https://paperswithcode.com/paper/slowtrack-increasing-the-latency-of-camera)]

- NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28253)] [[arxiv](https://arxiv.org/abs/2305.14836)] [[paper with code](https://paperswithcode.com/paper/nuscenes-qa-a-multi-modal-visual-question)] [[code](https://github.com/qiantianwen/nuscenes-qa)]

- DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28370)] [[arxiv](https://arxiv.org/abs/2304.01168)] [[paper with code](https://paperswithcode.com/paper/deepaccident-a-motion-and-accident-prediction)]

- Regulating Intermediate 3D Features for Vision-Centric Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28449)] [[arxiv](https://arxiv.org/abs/2312.11837)] [[paper with code](https://paperswithcode.com/paper/regulating-intermediate-3d-features-for)] [[code](https://github.com/cskkxjk/vampire)]

- BAT: Behavior-Aware Human-Like Trajectory Prediction for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28900)] [[arxiv](https://arxiv.org/abs/2312.06371)] [[paper with code](https://paperswithcode.com/paper/bat-behavior-aware-human-like-trajectory)] [[code](https://github.com/petrichor625/batraj-behavior-aware-model)]

- RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29049)]

- CCTR: Calibrating Trajectory Prediction for Uncertainty-Aware Motion Planning in Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/30085)]

- Reward (Mis)design for Autonomous Driving (Abstract Reprint) [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/30602)]



# 2023


## NeurIPS-2023


- Online Map Vectorization for Autonomous Driving: A Rasterization Perspective [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/654f61ecd998c9095d30d42c03b832aa-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2306.10502)] [[paper with code](https://paperswithcode.com/paper/online-map-vectorization-for-autonomous)] [[code](https://github.com/zhanggongjie/mapvr)] [[openreview](https://openreview.net/forum?id=YvO5yTVv5Y)]

- AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/95ab5c3e26fd82c7de3230bbad087d2d-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2306.00612)] [[paper with code](https://paperswithcode.com/paper/ad-pt-autonomous-driving-pre-training-with-1)] [[code](https://github.com/pjlab-adg/3dtrans)] [[openreview](https://openreview.net/forum?id=eIFZtkshgH)]

- What Truly Matters in Trajectory Prediction for Autonomous Driving? [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e197fe307eb3467035f892dc100d570a-Abstract-Conference.html)] [[paper with code](https://paperswithcode.com/paper/what-truly-matters-in-trajectory-prediction)] [[openreview](https://openreview.net/forum?id=nG35q8pNL9)]

- Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1838feeb71c4b4ea524d0df2f7074245-Abstract-Datasets_and_Benchmarks.html)] [[arxiv](https://arxiv.org/abs/2310.08710)] [[paper with code](https://paperswithcode.com/paper/waymax-an-accelerated-data-driven-simulator)] [[openreview](https://openreview.net/forum?id=7VSBaP2OXN)]

- Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/cabfaeecaae7d6540ee797a66f0130b0-Abstract-Datasets_and_Benchmarks.html)] [[arxiv](https://arxiv.org/abs/2304.14365)] [[paper with code](https://paperswithcode.com/paper/occ3d-a-large-scale-3d-occupancy-prediction-1)] [[code](https://github.com/Tsinghua-MARS-Lab/Occ3D)] [[openreview](https://openreview.net/forum?id=ApqgcSnhjh)]


## CVPR-2023


- MSeg3D: Multi-Modal 3D Semantic Segmentation for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_MSeg3D_Multi-Modal_3D_Semantic_Segmentation_for_Autonomous_Driving_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.08600)] [[paper with code](https://paperswithcode.com/paper/mseg3d-multi-modal-3d-semantic-segmentation)] [[code](https://github.com/jialeli1/lidarseg3d)]

- Localized Semantic Feature Mixers for Efficient Pedestrian Detection in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Khan_Localized_Semantic_Feature_Mixers_for_Efficient_Pedestrian_Detection_in_Autonomous_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/localized-semantic-feature-mixers-for)]

- Unsupervised 3D Point Cloud Representation Learning by Triangle Constrained Contrast for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pang_Unsupervised_3D_Point_Cloud_Representation_Learning_by_Triangle_Constrained_Contrast_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/unsupervised-3d-point-cloud-representation)]

- Azimuth Super-Resolution for FMCW Radar in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Azimuth_Super-Resolution_for_FMCW_Radar_in_Autonomous_Driving_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/azimuth-super-resolution-for-fmcw-radar-in)]

- RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Ando_RangeViT_Towards_Vision_Transformers_for_3D_Semantic_Segmentation_in_Autonomous_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2301.10222)] [[paper with code](https://paperswithcode.com/paper/rangevit-towards-vision-transformers-for-3d)] [[code](https://github.com/valeoai/rangevit)]

- Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/weakly-supervised-class-agnostic-motion)]

- TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Fang_TBP-Former_Learning_Temporal_Birds-Eye-View_Pyramid_for_Joint_Perception_and_Prediction_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.09998)] [[paper with code](https://paperswithcode.com/paper/tbp-former-learning-temporal-bird-s-eye-view)]

- Think Twice Before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2305.06242)] [[paper with code](https://paperswithcode.com/paper/think-twice-before-driving-towards-scalable)] [[code](https://github.com/opendrivelab/thinktwice)]

- Neural Map Prior for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Xiong_Neural_Map_Prior_for_Autonomous_Driving_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2304.08481)] [[paper with code](https://paperswithcode.com/paper/neural-map-prior-for-autonomous-driving)]

- Planning-Oriented Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2212.10156)] [[paper with code](https://paperswithcode.com/paper/goal-oriented-autonomous-driving)] [[code](https://github.com/opendrivelab/uniad)]

- Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liang_Visual_Exemplar_Driven_Task-Prompting_for_Unified_Perception_in_Autonomous_Driving_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.01788)] [[paper with code](https://paperswithcode.com/paper/visual-exemplar-driven-task-prompting-for)]

- Temporal Consistent 3D LiDAR Representation Learning for Semantic Perception in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Nunes_Temporal_Consistent_3D_LiDAR_Representation_Learning_for_Semantic_Perception_in_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/temporal-consistent-3d-lidar-representation)] [[code](https://github.com/prbonn/tarl)]

- Understanding the Robustness of 3D Object Detection With Bird's-Eye-View Representations in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Understanding_the_Robustness_of_3D_Object_Detection_With_Birds-Eye-View_Representations_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.17297)] [[paper with code](https://paperswithcode.com/paper/understanding-the-robustness-of-3d-object)] [[code](https://github.com/zzj403/BEV_Robust)]


## ICLR-2023


- CO3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving [[paper](https://iclr.cc/virtual/2023/poster/11868)] [[openreview](https://openreview.net/forum?id=QUaDoIdgo0)]

- Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling [[paper](https://iclr.cc/virtual/2023/poster/11975)] [[arxiv](https://arxiv.org/abs/2301.01006)] [[paper with code](https://paperswithcode.com/paper/policy-pre-training-for-end-to-end-autonomous)] [[code](https://github.com/opendrivelab/ppgeo)] [[openreview](https://openreview.net/forum?id=X5SUR7g2vVw)]


## ICCV-2023


- GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.05760)]

- Domain Generalization of 3D Semantic Segmentation in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Sanchez_Domain_Generalization_of_3D_Semantic_Segmentation_in_Autonomous_Driving_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2212.04245)] [[paper with code](https://paperswithcode.com/paper/domain-generalization-of-3d-semantic)] [[code](https://github.com/julessanchez/3dlabelprop)]

- Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2309.14491)] [[paper with code](https://paperswithcode.com/paper/unsupervised-3d-perception-with-2d-vision)]

- SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_SemARFlow_Injecting_Semantics_into_Unsupervised_Optical_Flow_Estimation_for_Autonomous_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.06209)] [[paper with code](https://paperswithcode.com/paper/semarflow-injecting-semantics-into)] [[code](https://github.com/duke-vision/semantic-unsup-flow-release)]

- Zenseact Open Dataset: A Large-Scale and Diverse Multimodal Dataset for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Alibeigi_Zenseact_Open_Dataset_A_Large-Scale_and_Diverse_Multimodal_Dataset_for_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2305.02008)] [[paper with code](https://paperswithcode.com/paper/zenseact-open-dataset-a-large-scale-and)] [[code](https://github.com/zenseact/zod)]

- VAD: Vectorized Scene Representation for Efficient Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.12077)] [[paper with code](https://paperswithcode.com/paper/vad-vectorized-scene-representation-for)] [[code](https://github.com/hustvl/vad)]

- Optimizing the Placement of Roadside LiDARs for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Optimizing_the_Placement_of_Roadside_LiDARs_for_Autonomous_Driving_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2310.07247)] [[paper with code](https://paperswithcode.com/paper/optimizing-the-placement-of-roadside-lidars-1)]

- DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.00398)] [[paper with code](https://paperswithcode.com/paper/driveadapter-breaking-the-coupling-barrier-of)] [[code](https://github.com/opendrivelab/driveadapter)]

- Learning Human Dynamics in Autonomous Driving Scenarios [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/learning-human-dynamics-in-autonomous-driving)]

- MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Liu_MV-DeepSDF_Implicit_Modeling_with_Multi-Sweep_Point_Clouds_for_3D_Vehicle_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2309.16715)] [[paper with code](https://paperswithcode.com/paper/mv-deepsdf-implicit-modeling-with-multi-sweep-1)]

- SurroundOcc: Multi-camera 3D Occupancy Prediction for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2303.09551)] [[paper with code](https://paperswithcode.com/paper/surroundocc-multi-camera-3d-occupancy)] [[code](https://github.com/weiyithu/surroundocc)]

- Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Does_Physical_Adversarial_Example_Really_Matter_to_Autonomous_Driving_Towards_ICCV_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/does-physical-adversarial-example-really)]

- Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Video_Task_Decathlon_Unifying_Image_and_Video_Tasks_in_Autonomous_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2309.04422)] [[paper with code](https://paperswithcode.com/paper/video-task-decathlon-unifying-image-and-video)]


## ICML-2023


- Transcendental Idealism of Planner: Evaluating Perception from Planning Perspective for Autonomous Driving [[paper](https://proceedings.mlr.press/v202/li23al.html)] [[arxiv](https://arxiv.org/abs/2306.07276)] [[paper with code](https://paperswithcode.com/paper/transcendental-idealism-of-planner-evaluating)] [[code](https://github.com/qcraftai/tip)] [[openreview](https://openreview.net/forum?id=E08kaoSiQl0)]


## IJCAI-2023


- CVTP3D: Cross-view Trajectory Prediction Using Shared 3D Queries for Autonomous Driving [[paper](https://www.ijcai.org/proceedings/2023/34)]

- DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving [[paper](https://www.ijcai.org/proceedings/2023/90)] [[arxiv](https://arxiv.org/abs/2303.17144)] [[paper with code](https://paperswithcode.com/paper/damo-streamnet-optimizing-streaming)] [[code](https://github.com/zhiqic/DAMO-StreamNet)]


## AAAI-2023


- LWSIS: LiDAR-Guided Weakly Supervised Instance Segmentation for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25228)] [[arxiv](https://arxiv.org/abs/2212.03504)] [[paper with code](https://paperswithcode.com/paper/lwsis-lidar-guided-weakly-supervised-instance)] [[code](https://github.com/serenos/lwsis)]

- READ: Large-Scale Neural Scene Rendering for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25238)] [[arxiv](https://arxiv.org/abs/2205.05509)] [[paper with code](https://paperswithcode.com/paper/read-large-scale-neural-scene-rendering-for)] [[code](https://github.com/JOP-Lee/READ-Large-Scale-Neural-Scene-Rendering-for-Autonomous-Driving)]

- Transformation-Equivariant 3D Object Detection for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25380)] [[arxiv](https://arxiv.org/abs/2211.11962)] [[paper with code](https://paperswithcode.com/paper/transformation-equivariant-3d-object)]



# 2022


## NeurIPS-2022


- Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/286a371d8a0a559281f682f8fbf89834-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2206.08129)] [[paper with code](https://paperswithcode.com/paper/trajectory-guided-control-prediction-for-end)] [[code](https://github.com/OpenPerceptionX/TCP)] [[openreview](https://openreview.net/forum?id=DhmYYrH_M3m)]

- Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7c319b62e2257b34cb0e1040ced2e007-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2209.08953)] [[paper with code](https://paperswithcode.com/paper/effective-adaptation-in-multi-task-co)] [[openreview](https://openreview.net/forum?id=HwP4XJ04Je1)]

- Unsupervised Adaptation from Repeated Traversals for Autonomous Driving [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1eb88348ee19a33c81cf5bc3fb8e9d2-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2303.15286)]

- K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/185fdf627eaae2abab36205dcd19b817-Abstract-Datasets_and_Benchmarks.html)] [[arxiv](https://arxiv.org/abs/2206.08171)] [[paper with code](https://paperswithcode.com/paper/k-radar-4d-radar-object-detection-dataset-and)] [[code](https://github.com/kaist-avelab/k-radar)] [[openreview](https://openreview.net/forum?id=W_bsDmzwaZ7)]


## CVPR-2022


- Exploiting Temporal Relations on Radar Perception for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Exploiting_Temporal_Relations_on_Radar_Perception_for_Autonomous_Driving_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2204.01184)] [[paper with code](https://paperswithcode.com/paper/exploiting-temporal-relations-on-radar)]

- LTP: Lane-Based Trajectory Prediction for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_LTP_Lane-Based_Trajectory_Prediction_for_Autonomous_Driving_CVPR_2022_paper.html)] [[paper with code](https://paperswithcode.com/paper/ltp-lane-based-trajectory-prediction-for)]

- Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Sautier_Image-to-Lidar_Self-Supervised_Distillation_for_Autonomous_Driving_Data_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2203.16258)] [[paper with code](https://paperswithcode.com/paper/image-to-lidar-self-supervised-distillation)] [[code](https://github.com/valeoai/slidr)]

- Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Time3D_End-to-End_Joint_Monocular_3D_Object_Detection_and_Tracking_for_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2205.14882)] [[paper with code](https://paperswithcode.com/paper/time3d-end-to-end-joint-monocular-3d-object)]

- Unifying Panoptic Segmentation for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zendel_Unifying_Panoptic_Segmentation_for_Autonomous_Driving_CVPR_2022_paper.html)] [[paper with code](https://paperswithcode.com/paper/unifying-panoptic-segmentation-for-autonomous)]

- Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Hu_Investigating_the_Impact_of_Multi-LiDAR_Placement_on_Object_Detection_for_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2105.00373)] [[paper with code](https://paperswithcode.com/paper/improving-perception-via-sensor-placement)] [[code](https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection)]

- Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Pseudo-Stereo_for_Monocular_3D_Object_Detection_in_Autonomous_Driving_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2203.02112)] [[paper with code](https://paperswithcode.com/paper/pseudo-stereo-for-monocular-3d-object)] [[code](https://github.com/revisitq/Pseudo-Stereo-3D)]

- Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Ye_Rope3D_The_Roadside_Perception_Dataset_for_Autonomous_Driving_and_Monocular_CVPR_2022_paper.html)] [[paper with code](https://paperswithcode.com/paper/rope3d-the-roadside-perception-dataset-for)]


## ECCV-2022


- Point Cloud Compression with Range Image-Based Entropy Model for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3885_ECCV_2022_paper.php)]

- Self-Distillation for Robust LiDAR Semantic Segmentation in Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/652_ECCV_2022_paper.php)]

- InAction: Interpretable Action Decision Making for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1109_ECCV_2022_paper.php)]

- CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1247_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2203.07724)] [[paper with code](https://paperswithcode.com/paper/coda-a-real-world-road-corner-case-dataset)]

- Motion Inspired Unsupervised Perception and Prediction in Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1257_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2210.08061)] [[paper with code](https://paperswithcode.com/paper/motion-inspired-unsupervised-perception-and)]

- ST-P3: End-to-End Vision-Based Autonomous Driving via Spatial-Temporal Feature Learning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1807_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2207.07601)] [[paper with code](https://paperswithcode.com/paper/st-p3-end-to-end-vision-based-autonomous)] [[code](https://github.com/opendrivelab/st-p3)]

- Rethinking Closed-Loop Training for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7315_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2306.15713)] [[paper with code](https://paperswithcode.com/paper/rethinking-closed-loop-training-for)]


## AAAI-2022


- Learning Mixture of Domain-Specific Experts via Disentangled Factors for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20000)] [[paper with code](https://paperswithcode.com/paper/learning-mixture-of-domain-specific-experts)] [[code](https://github.com/kimna4/MoDE)]

- Cross-Dataset Collaborative Learning for Semantic Segmentation in Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20149)] [[arxiv](https://arxiv.org/abs/2103.11351)] [[paper with code](https://paperswithcode.com/paper/cross-dataset-collaborative-learning-for)]



# 2021


## CVPR-2021


- Self-Supervised Pillar Motion Learning for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Self-Supervised_Pillar_Motion_Learning_for_Autonomous_Driving_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2104.08683)] [[paper with code](https://paperswithcode.com/paper/self-supervised-pillar-motion-learning-for)] [[code](https://github.com/qcraftai/pillar-motion)]

- Multi-Modal Fusion Transformer for End-to-End Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2104.09224)] [[paper with code](https://paperswithcode.com/paper/multi-modal-fusion-transformer-for-end-to-end)] [[code](https://github.com/autonomousvision/transfuser)]

- Shared Cross-Modal Trajectory Prediction for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Choi_Shared_Cross-Modal_Trajectory_Prediction_for_Autonomous_Driving_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2011.08436)] [[paper with code](https://paperswithcode.com/paper/shared-cross-modal-trajectory-prediction-for-1)]


## ICCV-2021


- Exploring Simple 3D Multi-Object Tracking for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Luo_Exploring_Simple_3D_Multi-Object_Tracking_for_Autonomous_Driving_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2108.10312)] [[paper with code](https://paperswithcode.com/paper/exploring-simple-3d-multi-object-tracking-for)] [[code](https://github.com/qcraftai/simtrack)]

- NEAT: Neural Attention Fields for End-to-End Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2109.04456)] [[paper with code](https://paperswithcode.com/paper/neat-neural-attention-fields-for-end-to-end)] [[code](https://github.com/autonomousvision/neat)]

- TMCOSS: Thresholded Multi-Criteria Online Subset Selection for Data-Efficient Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Das_TMCOSS_Thresholded_Multi-Criteria_Online_Subset_Selection_for_Data-Efficient_Autonomous_Driving_ICCV_2021_paper.html)] [[paper with code](https://paperswithcode.com/paper/tmcoss-thresholded-multi-criteria-online)]

- Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Ettinger_Large_Scale_Interactive_Motion_Forecasting_for_Autonomous_Driving_The_Waymo_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2104.10133)] [[paper with code](https://paperswithcode.com/paper/large-scale-interactive-motion-forecasting-1)]

- MultiSiam: Self-Supervised Multi-Instance Siamese Representation Learning for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_MultiSiam_Self-Supervised_Multi-Instance_Siamese_Representation_Learning_for_Autonomous_Driving_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2108.12178)] [[paper with code](https://paperswithcode.com/paper/multisiam-self-supervised-multi-instance)] [[code](https://github.com/kaichen1998/multisiam)]

- Safety-Aware Motion Prediction With Unseen Vehicles for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Ren_Safety-Aware_Motion_Prediction_With_Unseen_Vehicles_for_Autonomous_Driving_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2109.01510)] [[paper with code](https://paperswithcode.com/paper/safety-aware-motion-prediction-with-unseen)] [[code](https://github.com/xrenaa/safety-aware-motion-prediction)]

- MGNet: Monocular Geometric Scene Understanding for Autonomous Driving [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Schon_MGNet_Monocular_Geometric_Scene_Understanding_for_Autonomous_Driving_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2206.13199)] [[paper with code](https://paperswithcode.com/paper/mgnet-monocular-geometric-scene-understanding-1)] [[code](https://github.com/markusschoen/mgnet)]


## ICML-2021


- Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions [[paper](https://proceedings.mlr.press/v139/li21r.html)]


## AAAI-2021


- RTS3D: Real-time Stereo 3D Detection from 4D Feature-Consistency Embedding Space for Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16288)] [[arxiv](https://arxiv.org/abs/2012.15072)] [[paper with code](https://paperswithcode.com/paper/rts3d-real-time-stereo-3d-detection-from-4d)] [[code](https://github.com/Banconxuan/RTS3D)]

- Solution Concepts in Hierarchical Games Under Bounded Rationality With Applications to Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16715)] [[arxiv](https://arxiv.org/abs/2009.10033)] [[paper with code](https://paperswithcode.com/paper/solution-concepts-in-hierarchical-games-with)] [[code](https://git.uwaterloo.ca/a9sarkar/traffic_behavior_modeling)]



# 2020


## CVPR-2020


- Scalability in Perception for Autonomous Driving: Waymo Open Dataset [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/1912.04838)] [[paper with code](https://paperswithcode.com/paper/scalability-in-perception-for-autonomous)] [[code](https://github.com/open-mmlab/mmdetection3d)]

- Joint 3D Instance Segmentation and Object Detection for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_Joint_3D_Instance_Segmentation_and_Object_Detection_for_Autonomous_Driving_CVPR_2020_paper.html)] [[paper with code](https://paperswithcode.com/paper/joint-3d-instance-segmentation-and-object)]

- IDA-3D: Instance-Depth-Aware 3D Object Detection From Stereo Vision for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Peng_IDA-3D_Instance-Depth-Aware_3D_Object_Detection_From_Stereo_Vision_for_Autonomous_CVPR_2020_paper.html)] [[paper with code](https://paperswithcode.com/paper/ida-3d-instance-depth-aware-3d-object)] [[code](https://github.com/swords123/IDA-3D)]

- SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_SurfelGAN_Synthesizing_Realistic_Sensor_Data_for_Autonomous_Driving_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/2005.03844)] [[paper with code](https://paperswithcode.com/paper/surfelgan-synthesizing-realistic-sensor-data)]

- MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_MotionNet_Joint_Perception_and_Motion_Prediction_for_Autonomous_Driving_Based_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/2003.06754)] [[paper with code](https://paperswithcode.com/paper/motionnet-joint-perception-and-motion)] [[code](https://github.com/pxiangwu/MotionNet)]

- Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.html)] [[paper with code](https://paperswithcode.com/paper/exploring-data-aggregation-in-policy-learning)] [[code](https://github.com/autonomousvision/data_aggregation)]

- PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Kong_PhysGAN_Generating_Physical-World-Resilient_Adversarial_Examples_for_Autonomous_Driving_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/1907.04449)] [[paper with code](https://paperswithcode.com/paper/generating-adversarial-fragments-with)]

- Looking at the Right Stuff - Guided Semantic-Gaze for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/1911.10455)] [[paper with code](https://paperswithcode.com/paper/looking-at-the-right-stuff-guided-semantic-1)]

- nuScenes: A Multimodal Dataset for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/1903.11027)] [[paper with code](https://paperswithcode.com/paper/nuscenes-a-multimodal-dataset-for-autonomous)] [[code](https://github.com/nutonomy/nuscenes-devkit)]


## ICLR-2020


- Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving [[paper](https://iclr.cc/virtual/2020/poster/1727)] [[arxiv](https://arxiv.org/abs/1906.06310)] [[paper with code](https://paperswithcode.com/paper/pseudo-lidar-accurate-depth-for-3d-object)] [[code](https://github.com/mileyan/Pseudo_Lidar_V2)] [[openreview](https://openreview.net/forum?id=BJedHRVtPB)]


## ECCV-2020


- RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1054_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2001.03343)]

- Active Perception using Light Curtains for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4458_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2008.02191)]

- InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1137_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2007.08556)]

- DVI: Depth Guided Video Inpainting for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3620_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2007.08854)]

- PiP: Planning-informed Trajectory Prediction for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3805_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2003.11476)]

- Pillar-based Object Detection for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3892_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2007.10323)]

- DA4AD: End-to-End Deep Attention-based Visual Localization for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6091_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2003.03026)]


## AAAI-2020


- A Novel Learning Framework for Sampling-Based Motion Planning in Autonomous Driving [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/5473)]

- AutoRemover: Automatic Object Removal for Autonomous Driving Videos [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6982)] [[arxiv](https://arxiv.org/abs/1911.12588)] [[paper with code](https://paperswithcode.com/paper/autoremover-automatic-object-removal-for)] [[code](https://github.com/zrfreya/AutoRemover)]



# 2019


## CVPR-2019


- DrivingStereo: A Large-Scale Dataset for Stereo Matching in Autonomous Driving Scenarios [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_DrivingStereo_A_Large-Scale_Dataset_for_Stereo_Matching_in_Autonomous_Driving_CVPR_2019_paper.html)] [[paper with code](https://paperswithcode.com/paper/drivingstereo-a-large-scale-dataset-for)]

- GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_GS3D_An_Efficient_3D_Object_Detection_Framework_for_Autonomous_Driving_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1903.10955)] [[paper with code](https://paperswithcode.com/paper/gs3d-an-efficient-3d-object-detection)]

- ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Song_ApolloCar3D_A_Large_3D_Car_Instance_Understanding_Benchmark_for_Autonomous_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1811.12222)] [[paper with code](https://paperswithcode.com/paper/apollocar3d-a-large-3d-car-instance)]

- L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Lu_L3-Net_Towards_Learning_Based_LiDAR_Localization_for_Autonomous_Driving_CVPR_2019_paper.html)] [[paper with code](https://paperswithcode.com/paper/l3-net-towards-learning-based-lidar)]

- Stereo R-CNN Based 3D Object Detection for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Stereo_R-CNN_Based_3D_Object_Detection_for_Autonomous_Driving_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1902.09738)] [[paper with code](https://paperswithcode.com/paper/stereo-r-cnn-based-3d-object-detection-for)] [[code](https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN)]

- Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Pseudo-LiDAR_From_Visual_Depth_Estimation_Bridging_the_Gap_in_3D_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1812.07179)] [[paper with code](https://paperswithcode.com/paper/pseudo-lidar-from-visual-depth-estimation)] [[code](https://github.com/mileyan/pseudo_lidar)]

- LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Meyer_LaserNet_An_Efficient_Probabilistic_3D_Object_Detector_for_Autonomous_Driving_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1903.08701)] [[paper with code](https://paperswithcode.com/paper/lasernet-an-efficient-probabilistic-3d-object)]


## ICCV-2019


- Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1904.04620)] [[paper with code](https://paperswithcode.com/paper/gaussian-yolov3-an-accurate-and-fast-object)] [[code](https://github.com/jwchoi384/Gaussian_YOLOv3)]

- Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Accurate_Monocular_3D_Object_Detection_via_Color-Embedded_3D_Reconstruction_for_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1903.11444)] [[paper with code](https://paperswithcode.com/paper/accurate-monocular-3d-object-detection-via-1)]

- WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Yogamani_WoodScape_A_Multi-Task_Multi-Camera_Fisheye_Dataset_for_Autonomous_Driving_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1905.01489)] [[paper with code](https://paperswithcode.com/paper/woodscape-a-multi-task-multi-camera-fisheye)] [[code](https://github.com/valeoai/WoodScape)]

- Scalable Place Recognition Under Appearance Change for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Doan_Scalable_Place_Recognition_Under_Appearance_Change_for_Autonomous_Driving_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1908.00178)] [[paper with code](https://paperswithcode.com/paper/scalable-place-recognition-under-appearance)]

- Exploring the Limitations of Behavior Cloning for Autonomous Driving [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Codevilla_Exploring_the_Limitations_of_Behavior_Cloning_for_Autonomous_Driving_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1904.08980)] [[paper with code](https://paperswithcode.com/paper/exploring-the-limitations-of-behavior-cloning)] [[code](https://github.com/felipecode/coiltraine)]


## IJCAI-2019


- Out of Sight But Not Out of Mind: An Answer Set Programming Based Online Abduction Framework for Visual Sensemaking in Autonomous Driving [[paper](https://www.ijcai.org/proceedings/2019/260)] [[arxiv](https://arxiv.org/abs/1906.00107)] [[paper with code](https://paperswithcode.com/paper/190600107)]

- Randomized Adversarial Imitation Learning for Autonomous Driving [[paper](https://www.ijcai.org/proceedings/2019/638)] [[arxiv](https://arxiv.org/abs/1905.05637)] [[paper with code](https://paperswithcode.com/paper/randomized-adversarial-imitation-learning-for)]

- Failure-Scenario Maker for Rule-Based Agent using Multi-agent Adversarial Reinforcement Learning and its Application to Autonomous Driving [[paper](https://www.ijcai.org/proceedings/2019/832)] [[arxiv](https://arxiv.org/abs/1903.10654)] [[paper with code](https://paperswithcode.com/paper/failure-scenario-maker-for-rule-based-agent)]


# 2018


## ECCV-2018


- Real-to-Virtual Domain Unification for End-to-End Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2018/papers_ECCV/html/Luona_Yang_Real-to-Virtual_Domain_Uni_ECCV_2018_paper.php)] [[arxiv](https://arxiv.org/abs/1801.03458)] [[paper with code](https://paperswithcode.com/paper/real-to-virtual-domain-unification-for-end-to)]

- Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving [[paper](https://www.ecva.net/papers/eccv_2018/papers_ECCV/html/Peiliang_LI_Stereo_Vision-based_Semantic_ECCV_2018_paper.php)] [[arxiv](https://arxiv.org/abs/1807.02062)] [[paper with code](https://paperswithcode.com/paper/stereo-vision-based-semantic-3d-object-and)]