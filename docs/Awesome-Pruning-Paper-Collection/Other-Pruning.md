# Awesome Other Pruning Paper Collection


- [2024](#2024)
  - [ICLR](#iclr-2024)
  - [AAAI](#aaai-2024)

- [2023](#2023)
  - [NeurIPS](#neurips-2023)
  - [CVPR](#cvpr-2023)
  - [ICLR](#iclr-2023)
  - [ICCV](#iccv-2023)
  - [ICML](#icml-2023)
  - [ACL](#acl-2023)
  - [IJCAI](#ijcai-2023)
  - [AAAI](#aaai-2023)

- [2022](#2022)
  - [NeurIPS](#neurips-2022)
  - [CVPR](#cvpr-2022)
  - [ICLR](#iclr-2022)
  - [ECCV](#eccv-2022)
  - [ICML](#icml-2022)
  - [ACL](#acl-2022)
  - [IJCAI](#ijcai-2022)
  - [AAAI](#aaai-2022)

- [2021](#2021)
  - [NeurIPS](#neurips-2021)
  - [CVPR](#cvpr-2021)
  - [ICLR](#iclr-2021)
  - [ICCV](#iccv-2021)
  - [ICML](#icml-2021)
  - [ACL](#acl-2021)
  - [IJCAI](#ijcai-2021)
  - [AAAI](#aaai-2021)

- [2020](#2020)
  - [NeurIPS](#neurips-2020)
  - [CVPR](#cvpr-2020)
  - [ICLR](#iclr-2020)
  - [ECCV](#eccv-2020)
  - [ICML](#icml-2020)
  - [IJCAI](#ijcai-2020)
  - [AAAI](#aaai-2020)

- [2019](#2019)
  - [NeurIPS](#neurips-2019)
  - [CVPR](#cvpr-2019)
  - [ICLR](#iclr-2019)
  - [ICCV](#iccv-2019)
  - [ICML](#icml-2019)
  - [ACL](#acl-2019)
  - [IJCAI](#ijcai-2019)
  - [AAAI](#aaai-2019)

- [2018](#2018)
  - [NeurIPS](#neurips-2018)
  - [CVPR](#cvpr-2018)
  - [ICLR](#iclr-2018)
  - [ECCV](#eccv-2018)
  - [IJCAI](#ijcai-2018)
  - [AAAI](#aaai-2018)



# 2024


## ICLR-2024


- FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity [[paper](https://iclr.cc/virtual/2024/poster/18092)] [[openreview](https://openreview.net/forum?id=hbHwZYqk9T)]

- A Simple and Effective Pruning Approach for Large Language Models [[paper](https://iclr.cc/virtual/2024/poster/18687)] [[arxiv](https://arxiv.org/abs/2306.11695)] [[paper with code](https://paperswithcode.com/paper/a-simple-and-effective-pruning-approach-for)] [[code](https://github.com/locuslab/wanda)]

- Effective pruning of web-scale datasets based on complexity of concept clusters [[paper](https://iclr.cc/virtual/2024/poster/19159)] [[arxiv](https://arxiv.org/abs/2401.04578)] [[paper with code](https://paperswithcode.com/paper/effective-pruning-of-web-scale-datasets-based)] [[code](https://github.com/amro-kamal/effective_pruning)] [[openreview](https://openreview.net/forum?id=CtOA9aN8fr)]

- Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN [[paper](https://iclr.cc/virtual/2024/poster/19606)] [[arxiv](https://arxiv.org/abs/2403.03409)] [[paper with code](https://paperswithcode.com/paper/sparse-spiking-neural-network-exploiting)] [[openreview](https://openreview.net/forum?id=0jsfesDZDq)]

- Adaptive Window Pruning for Efficient Local Motion Deblurring [[paper](https://iclr.cc/virtual/2024/poster/18103)] [[arxiv](https://arxiv.org/abs/2306.14268)] [[paper with code](https://paperswithcode.com/paper/adaptive-window-pruning-for-efficient-local)] [[openreview](https://openreview.net/forum?id=hI18CDyadM)]

- Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models [[paper](https://iclr.cc/virtual/2024/poster/17667)] [[arxiv](https://arxiv.org/abs/2308.03449)] [[paper with code](https://paperswithcode.com/paper/knowledge-preserving-pruning-for-pre-trained)] [[code](https://github.com/snudm-starlab/k-prune)] [[openreview](https://openreview.net/forum?id=s2NjWfaYdZ)]

- Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning [[paper](https://iclr.cc/virtual/2024/poster/19623)] [[arxiv](https://arxiv.org/abs/2310.06694)] [[paper with code](https://paperswithcode.com/paper/sheared-llama-accelerating-language-model-pre)] [[code](https://github.com/princeton-nlp/llm-shearing)] [[openreview](https://openreview.net/forum?id=09iOdaeOzp)]

- Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning [[paper](https://iclr.cc/virtual/2024/poster/18434)] [[arxiv](https://arxiv.org/abs/2305.14852)] [[openreview](https://openreview.net/forum?id=Y9t7MqZtCR)]

- Adaptive Sharpness-Aware Pruning for Robust Sparse Networks [[paper](https://iclr.cc/virtual/2024/poster/18682)] [[arxiv](https://arxiv.org/abs/2306.14306)] [[paper with code](https://paperswithcode.com/paper/adaptive-sharpness-aware-pruning-for-robust)] [[openreview](https://openreview.net/forum?id=QFYVVwiAM8)]

- Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging [[paper](https://iclr.cc/virtual/2024/poster/17433)] [[arxiv](https://arxiv.org/abs/2306.16788)] [[paper with code](https://paperswithcode.com/paper/sparse-model-soups-a-recipe-for-improved)] [[code](https://github.com/zib-iol/sms)] [[openreview](https://openreview.net/forum?id=xx0ITyHp3u)]

- What Makes a Good Prune? Optimal Unstructured Pruning for Maximal Cosine Similarity [[paper](https://iclr.cc/virtual/2024/poster/18003)]

- SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning [[paper](https://iclr.cc/virtual/2024/poster/18868)] [[arxiv](https://arxiv.org/abs/2310.04918)] [[paper with code](https://paperswithcode.com/paper/robust-network-pruning-with-sparse-entropic)] [[code](https://github.com/youlei202/entropic-wasserstein-pruning)] [[openreview](https://openreview.net/forum?id=LJWizuuBUy)]

- Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning [[paper](https://iclr.cc/virtual/2024/poster/19052)] [[openreview](https://openreview.net/forum?id=Fk5IzauJ7F)]

- Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework [[paper](https://iclr.cc/virtual/2024/poster/18207)] [[openreview](https://openreview.net/forum?id=eoSeaK4QJo)]

- Towards Meta-Pruning via Optimal Transport [[paper](https://iclr.cc/virtual/2024/poster/17651)] [[arxiv](https://arxiv.org/abs/2402.07839)] [[paper with code](https://paperswithcode.com/paper/towards-meta-pruning-via-optimal-transport)] [[code](https://github.com/alexandertheus/intra-fusion)] [[openreview](https://openreview.net/forum?id=sMoifbuxjB)]

- $\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning [[paper](https://iclr.cc/virtual/2024/poster/17608)] [[openreview](https://openreview.net/forum?id=thbtoAkCe9)]

- BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation [[paper](https://iclr.cc/virtual/2024/poster/18153)] [[arxiv](https://arxiv.org/abs/2402.16880)] [[paper with code](https://paperswithcode.com/paper/besa-pruning-large-language-models-with)] [[code](https://github.com/opengvlab/llmprune-besa)] [[openreview](https://openreview.net/forum?id=gC6JTEU3jl)]

- ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models [[paper](https://iclr.cc/virtual/2024/poster/18067)] [[arxiv](https://arxiv.org/abs/2310.02998)] [[paper with code](https://paperswithcode.com/paper/ecoflap-efficient-coarse-to-fine-layer-wise)] [[openreview](https://openreview.net/forum?id=iIT02bAKzv)]

- Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models [[paper](https://iclr.cc/virtual/2024/poster/18549)] [[openreview](https://openreview.net/forum?id=Tr0lPx9woF)]

- Adversarial Feature Map Pruning for Backdoor [[paper](https://iclr.cc/virtual/2024/poster/18966)] [[arxiv](https://arxiv.org/abs/2307.11565)] [[paper with code](https://paperswithcode.com/paper/fmt-removing-backdoor-feature-maps-via)] [[code](https://github.com/retsuh-bqw/fmp)] [[openreview](https://openreview.net/forum?id=IOEEDkla96)]

- InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning [[paper](https://iclr.cc/virtual/2024/poster/19186)] [[arxiv](https://arxiv.org/abs/2303.04947)] [[paper with code](https://paperswithcode.com/paper/infobatch-lossless-training-speed-up-by)] [[code](https://github.com/nus-hpc-ai-lab/infobatch)] [[openreview](https://openreview.net/forum?id=C61sk5LsK6)]


## AAAI-2024


- Graph Context Transformation Learning for Progressive Correspondence Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/27967)] [[arxiv](https://arxiv.org/abs/2312.15971)] [[paper with code](https://paperswithcode.com/paper/graph-context-transformation-learning-for)]

- Entropy Induced Pruning Framework for Convolutional Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28184)] [[arxiv](https://arxiv.org/abs/2208.06660)] [[paper with code](https://paperswithcode.com/paper/entropy-induced-pruning-framework-for)]

- BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28218)] [[arxiv](https://arxiv.org/abs/2401.03459)] [[paper with code](https://paperswithcode.com/paper/bclnet-bilateral-consensus-learning-for-two)] [[code](https://github.com/guobaoxiao/BCLNet)]

- RG-GAN: Dynamic Regenerative Pruning for Data-Efficient Generative Adversarial Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28271)]

- FÂ³-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28300)]

- Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28460)] [[arxiv](https://arxiv.org/abs/2401.16687)] [[paper with code](https://paperswithcode.com/paper/revisiting-gradient-pruning-a-dual)]

- Dynamic Feature Pruning and Consolidation for Occluded Person Re-identification [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28491)] [[arxiv](https://arxiv.org/abs/2211.14742)] [[paper with code](https://paperswithcode.com/paper/dynamic-feature-pruning-and-consolidation-for)] [[code](https://github.com/babahui/fpc)]

- IRPruneDet: Efficient Infrared Small Target Detection via Wavelet Structure-Regularized Soft Channel Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28551)]

- Fluctuation-Based Adaptive Structured Pruning for Large Language Models [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28960)] [[arxiv](https://arxiv.org/abs/2312.11983)] [[paper with code](https://paperswithcode.com/paper/fluctuation-based-adaptive-structured-pruning)] [[code](https://github.com/casia-iva-lab/flap)]

- MEPSI: An MDL-Based Ensemble Pruning Approach with Structural Information [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/28984)]

- EPSD: Early Pruning with Self-Distillation for Efficient Model Compression [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29004)] [[arxiv](https://arxiv.org/abs/2402.00084)] [[paper with code](https://paperswithcode.com/paper/epsd-early-pruning-with-self-distillation-for)]

- REPrune: Channel Pruning via Kernel Representative Selection [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29370)] [[arxiv](https://arxiv.org/abs/2402.17862)] [[paper with code](https://paperswithcode.com/paper/reprune-channel-pruning-via-kernel)]



# 2023


## NeurIPS-2023


- Towards Higher Ranks via Adversarial Weight Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/040ace837dd270a87055bb10dd7c0392-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2311.17493)] [[paper with code](https://paperswithcode.com/paper/towards-higher-ranks-via-adversarial-weight-1)] [[code](https://github.com/huawei-noah/Efficient-Computing)] [[openreview](https://openreview.net/forum?id=Y17N9B0vXn)]

- Structural Pruning for Diffusion Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/35c1d69d23bb5dd6b9abcd68be005d5c-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2305.10924)] [[paper with code](https://paperswithcode.com/paper/structural-pruning-for-diffusion-models-1)] [[code](https://github.com/vainf/diff-pruning)] [[openreview](https://openreview.net/forum?id=d4f40zJJIS)]

- Data Pruning via Moving-one-Sample-out [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/3abe23bf7e295b44369c24465d68987a-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2310.14664)] [[paper with code](https://paperswithcode.com/paper/data-pruning-via-moving-one-sample-out)] [[openreview](https://openreview.net/forum?id=vO6ZdPWaHc)]

- Neural Sculpting: Uncovering hierarchically modular task structure in neural networks through pruning and network analysis [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/3b1675de6b49cc00084374213f8c38ae-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2305.18402)] [[openreview](https://openreview.net/forum?id=1jhmWkZGy6)]

- LLM-Pruner: On the Structural Pruning of Large Language Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/44956951349095f74492a5471128a7e0-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2305.11627)] [[paper with code](https://paperswithcode.com/paper/llm-pruner-on-the-structural-pruning-of-large-1)] [[code](https://github.com/horseee/llm-pruner)] [[openreview](https://openreview.net/forum?id=J8Ajf9WfXP)]

- CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5bd9fbb3a5a985f80c16ddd0ec1dfc43-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2210.09223)]

- Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/749252feedd44f7f10d47ec1d674a2f8-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2310.08782)] [[paper with code](https://paperswithcode.com/paper/selectivity-drives-productivity-efficient)] [[code](https://github.com/optml-group/dp4tl)] [[openreview](https://openreview.net/forum?id=D0MII7rP3R)]

- You Only Condense Once: Two Rules for Pruning Condensed Datasets [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/7bdd36a198a8408f444834039b09f518-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2310.14019)] [[paper with code](https://paperswithcode.com/paper/you-only-condense-once-two-rules-for-pruning-1)] [[code](https://github.com/he-y/you-only-condense-once)] [[openreview](https://openreview.net/forum?id=AlTyimRsLf)]

- PDP: Parameter-free Differentiable Pruning is All You Need [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8f9f4eb32b9081a90f2a0b2627eb2a24-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2305.11203)] [[paper with code](https://paperswithcode.com/paper/pdp-parameter-free-differentiable-pruning-is)] [[openreview](https://openreview.net/forum?id=lLztVBaBVU)]

- SUBP: Soft Uniform Block Pruning for 1$\times$N Sparse CNNs Multithreading Acceleration [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a36c3dbe676fa8445715a31a90c66ab3-Abstract-Conference.html)] [[paper with code](https://paperswithcode.com/paper/subp-soft-uniform-block-pruning-for-1-times-n)] [[code](https://github.com/JingyangXiang/SUBP)] [[openreview](https://openreview.net/forum?id=J0Pvvxspmz)]

- Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a4316bb210a59fb7aafeca5dd21c2703-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2402.10062)] [[paper with code](https://paperswithcode.com/paper/optimal-parameter-and-neuron-pruning-for-out-1)] [[openreview](https://openreview.net/forum?id=TtCPFN5fhO)]

- Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad02c6f3824f871395112ae71a28eff7-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2306.12230)] [[paper with code](https://paperswithcode.com/paper/fantastic-weights-and-how-to-find-them-where-1)] [[code](https://github.com/alooow/fantastic_weights_paper)] [[openreview](https://openreview.net/forum?id=hkPn7M9k1W)]

- Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/bd2107343c9cc973635d90dbfc122223-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2303.11453)] [[paper with code](https://paperswithcode.com/paper/greedy-pruning-with-group-lasso-provably)] [[openreview](https://openreview.net/forum?id=BTRcVP7ZJn)]

- One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c3aba4234afd1c8116d879ba183f4835-Abstract-Conference.html)] [[paper with code](https://paperswithcode.com/paper/one-less-reason-for-filter-pruning-gaining)] [[code](https://github.com/henryzhongsc/adv_robust_gkp)] [[openreview](https://openreview.net/forum?id=Pjky9XG8zP)]

- Pruning vs Quantization: Which is Better? [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c48bc80aa5d3cbbdd712d1cc107b8319-Abstract-Conference.html)] [[paper with code](https://paperswithcode.com/paper/pruning-vs-quantization-which-is-better)] [[code](https://github.com/Qualcomm-AI-research/pruning-vs-quantization)] [[openreview](https://openreview.net/forum?id=0OU1ZXXxs5)]

- Don't just prune by magnitude! Your mask topology is a secret weapon [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/cd5404354496e39d37b7947d8a0d7b72-Abstract-Conference.html)] [[paper with code](https://paperswithcode.com/paper/dont-just-prune-by-magnitude-your-mask)] [[code](https://github.com/vita-group/fullspectrum-pai)] [[openreview](https://openreview.net/forum?id=DIBcdjWV7k)]

- ZipLM: Inference-Aware Structured Pruning of Language Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ced46a50befedcb884ccf0cbe8c3ad23-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2302.04089)] [[paper with code](https://paperswithcode.com/paper/ziplm-inference-aware-structured-pruning-of-1)] [[code](https://github.com/ist-daslab/ziplm)] [[openreview](https://openreview.net/forum?id=d8j3lsBWpV)]

- Learning Large-scale Neural Fields via Context Pruned Meta-Learning [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e5b5c402bb7bd5e60bede6961d6fe39e-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2302.00617)] [[paper with code](https://paperswithcode.com/paper/learning-large-scale-neural-fields-via)] [[code](https://github.com/jihoontack/gradncp)] [[openreview](https://openreview.net/forum?id=QGmNMtK3pQ)]

- Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ebb6bee50913ba7e1efeb91a1d47a002-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2311.01002)] [[paper with code](https://paperswithcode.com/paper/robust-data-pruning-under-label-noise-via)] [[code](https://github.com/kaist-dmlab/Prune4Rel)] [[openreview](https://openreview.net/forum?id=xWCp0uLcpG)]

- Towards Data-Agnostic Pruning At Initialization: What Makes a Good Sparse Mask? [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd5013ea0c3f96931dec77174eaf9d80-Abstract-Conference.html)] [[paper with code](https://paperswithcode.com/paper/towards-data-agnostic-pruning-at)] [[code](https://github.com/pvh1602/npb)] [[openreview](https://openreview.net/forum?id=xdOoCWCYaY)]


## CVPR-2023


- Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Iofinova_Bias_in_Pruned_Vision_Models_In-Depth_Analysis_and_Countermeasures_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2304.12622)] [[paper with code](https://paperswithcode.com/paper/bias-in-pruned-vision-models-in-depth)]

- DepGraph: Towards Any Structural Pruning [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2301.12900)] [[paper with code](https://paperswithcode.com/paper/depgraph-towards-any-structural-pruning)] [[code](https://github.com/VainF/Torch-Pruning)]

- Progressive Neighbor Consistency Mining for Correspondence Pruning [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Progressive_Neighbor_Consistency_Mining_for_Correspondence_Pruning_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/progressive-neighbor-consistency-mining-for)] [[code](https://github.com/xinliu29/ncmnet)]

- Training Debiased Subnetworks With Contrastive Weight Pruning [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Park_Training_Debiased_Subnetworks_With_Contrastive_Weight_Pruning_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2210.05247)] [[paper with code](https://paperswithcode.com/paper/efficient-debiasing-with-contrastive-weight)] [[code](https://github.com/parkgeonyeong/dcwp)]

- Pruning Parameterization With Bi-Level Optimization for Efficient Semantic Segmentation on the Edge [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Pruning_Parameterization_With_Bi-Level_Optimization_for_Efficient_Semantic_Segmentation_on_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/pruning-parameterization-with-bi-level)]

- Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Out-of-Distributed_Semantic_Pruning_for_Robust_Semi-Supervised_Learning_CVPR_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2305.18158)] [[paper with code](https://paperswithcode.com/paper/out-of-distributed-semantic-pruning-for-1)] [[code](https://github.com/rain305f/osp)]

- CP3: Channel Pruning Plug-In for Point-Based Networks [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_CP3_Channel_Pruning_Plug-In_for_Point-Based_Networks_CVPR_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/cp3-channel-pruning-plug-in-for-point-based)]


## ICLR-2023


- Bit-Pruning: A Sparse Multiplication-Less Dot-Product [[paper](https://iclr.cc/virtual/2023/poster/12205)] [[openreview](https://openreview.net/forum?id=YUDiZcZTI8)]

- Trainability Preserving Neural Pruning [[paper](https://iclr.cc/virtual/2023/poster/12213)] [[arxiv](https://arxiv.org/abs/2207.12534)] [[paper with code](https://paperswithcode.com/paper/trainability-preserving-neural-structured)] [[code](https://github.com/mingsun-tse/tpp)] [[openreview](https://openreview.net/forum?id=AZFvpnnewr)]

- Holistic Adversarially Robust Pruning [[paper](https://iclr.cc/virtual/2023/poster/11204)] [[openreview](https://openreview.net/forum?id=sAJDi9lD06L)]

- Coverage-centric Coreset Selection for High Pruning Rates [[paper](https://iclr.cc/virtual/2023/poster/11710)] [[arxiv](https://arxiv.org/abs/2210.15809)] [[paper with code](https://paperswithcode.com/paper/coverage-centric-coreset-selection-for-high)] [[code](https://github.com/haizhongzheng/coverage-centric-coreset-selection)] [[openreview](https://openreview.net/forum?id=QwKvL6wC8Yi)]

- DFPC: Data flow driven pruning of coupled channels without data. [[paper](https://iclr.cc/virtual/2023/poster/10681)] [[openreview](https://openreview.net/forum?id=mhnHqRqcjYU)]

- Symmetric Pruning in Quantum Neural Networks [[paper](https://iclr.cc/virtual/2023/poster/11285)] [[arxiv](https://arxiv.org/abs/2208.14057)] [[paper with code](https://paperswithcode.com/paper/symmetric-pruning-in-quantum-neural-networks)] [[openreview](https://openreview.net/forum?id=K96AogLDT2K)]

- A Unified Framework for Soft Threshold Pruning [[paper](https://iclr.cc/virtual/2023/poster/12217)] [[arxiv](https://arxiv.org/abs/2302.13019)] [[paper with code](https://paperswithcode.com/paper/a-unified-framework-for-soft-threshold)] [[code](https://github.com/yanqi-chen/lats)] [[openreview](https://openreview.net/forum?id=cCFqcrq0d8)]

- TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning [[paper](https://iclr.cc/virtual/2023/poster/10682)] [[openreview](https://openreview.net/forum?id=sZI1Oj9KBKy)]

- Dataset Pruning: Reducing Training Data by Examining Generalization Influence [[paper](https://iclr.cc/virtual/2023/poster/12019)] [[arxiv](https://arxiv.org/abs/2205.09329)] [[paper with code](https://paperswithcode.com/paper/dataset-pruning-reducing-training-data-by)] [[openreview](https://openreview.net/forum?id=4wZiAXD29TQ)]

- Pruning Deep Neural Networks from a Sparsity Perspective [[paper](https://iclr.cc/virtual/2023/poster/10851)] [[arxiv](https://arxiv.org/abs/2302.05601)] [[paper with code](https://paperswithcode.com/paper/pruning-deep-neural-networks-from-a-sparsity-1)] [[code](https://github.com/dem123456789/Pruning-Deep-Neural-Networks-from-a-Sparsity-Perspective)] [[openreview](https://openreview.net/forum?id=i-DleYh34BM)]

- NTK-SAP: Improving neural network pruning by aligning training dynamics [[paper](https://iclr.cc/virtual/2023/poster/12107)] [[arxiv](https://arxiv.org/abs/2304.02840)] [[paper with code](https://paperswithcode.com/paper/ntk-sap-improving-neural-network-pruning-by)] [[code](https://github.com/yitewang/ntk-sap)] [[openreview](https://openreview.net/forum?id=-5EWhW_4qWP)]

- REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH [[paper](https://iclr.cc/virtual/2023/poster/11541)] [[openreview](https://openreview.net/forum?id=uVcDssQff_)]

- Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models [[paper](https://iclr.cc/virtual/2023/poster/11958)] [[openreview](https://openreview.net/forum?id=UMERaIHMwB3)]


## ICCV-2023


- HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Xie_HollowNeRF_Pruning_Hashgrid-Based_NeRFs_with_Trainable_Collision_Mitigation_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.10122)] [[paper with code](https://paperswithcode.com/paper/hollownerf-pruning-hashgrid-based-nerfs-with)]

- Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Efficient_Joint_Optimization_of_Layer-Adaptive_Weight_Pruning_in_Deep_Neural_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.10438)] [[paper with code](https://paperswithcode.com/paper/efficient-joint-optimization-of-layer)] [[code](https://github.com/Akimoto-Cris/RD_PRUNE)]

- Towards Fairness-aware Adversarial Network Pruning [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Towards_Fairness-aware_Adversarial_Network_Pruning_ICCV_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/towards-fairness-aware-adversarial-network)]

- Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.04549)] [[paper with code](https://paperswithcode.com/paper/prune-spatio-temporal-tokens-by-semantic)] [[code](https://github.com/mark12ding/sta)]

- Differentiable Transportation Pruning [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Differentiable_Transportation_Pruning_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2307.08483)] [[paper with code](https://paperswithcode.com/paper/differentiable-transportation-pruning)]

- Structural Alignment for Network Pruning through Partial Regularization [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Structural_Alignment_for_Network_Pruning_through_Partial_Regularization_ICCV_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/structural-alignment-for-network-pruning)]

- Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Bai_Unified_Data-Free_Compression_Pruning_and_Quantization_without_Fine-Tuning_ICCV_2023_paper.html)] [[arxiv](https://arxiv.org/abs/2308.07209)] [[paper with code](https://paperswithcode.com/paper/unified-data-free-compression-pruning-and)]

- Automatic Network Pruning via Hilbert-Schmidt Independence Criterion Lasso under Information Bottleneck Principle [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Automatic_Network_Pruning_via_Hilbert-Schmidt_Independence_Criterion_Lasso_under_Information_ICCV_2023_paper.html)] [[paper with code](https://paperswithcode.com/paper/automatic-network-pruning-via-hilbert-schmidt)] [[code](https://github.com/sunggo/apib)]


## ICML-2023


- Fast as CHITA: Neural Network Pruning with Combinatorial Optimization [[paper](https://proceedings.mlr.press/v202/benbaki23a.html)] [[arxiv](https://arxiv.org/abs/2302.14623)] [[paper with code](https://paperswithcode.com/paper/fast-as-chita-neural-network-pruning-with)]

- SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot [[paper](https://proceedings.mlr.press/v202/frantar23a.html)] [[arxiv](https://arxiv.org/abs/2301.00774)] [[paper with code](https://paperswithcode.com/paper/massive-language-models-can-be-accurately)] [[code](https://github.com/ist-daslab/sparsegpt)]

- Why Random Pruning Is All We Need to Start Sparse [[paper](https://proceedings.mlr.press/v202/gadhikar23a.html)] [[arxiv](https://arxiv.org/abs/2210.02412)] [[paper with code](https://paperswithcode.com/paper/how-erdos-and-renyi-win-the-lottery)] [[code](https://github.com/relationalml/sparse_to_sparse)]

- Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models [[paper](https://proceedings.mlr.press/v202/jaiswal23b.html)] [[arxiv](https://arxiv.org/abs/2306.10460)] [[paper with code](https://paperswithcode.com/paper/instant-soup-cheap-pruning-ensembles-in-a)] [[code](https://github.com/vita-group/instant_soup)]

- Reconstructive Neuron Pruning for Backdoor Defense [[paper](https://proceedings.mlr.press/v202/li23v.html)] [[arxiv](https://arxiv.org/abs/2305.14876)] [[paper with code](https://paperswithcode.com/paper/2305-14876)] [[code](https://github.com/bboylyg/rnp)]

- Pruning via Sparsity-indexed ODE: a Continuous Sparsity Viewpoint [[paper](https://proceedings.mlr.press/v202/mo23c.html)]

- Gradient-Free Structured Pruning with Unlabeled Data [[paper](https://proceedings.mlr.press/v202/nova23a.html)] [[arxiv](https://arxiv.org/abs/2303.04185)] [[paper with code](https://paperswithcode.com/paper/gradient-free-structured-pruning-with)]

- UPSCALE: Unconstrained Channel Pruning [[paper](https://proceedings.mlr.press/v202/wan23a.html)] [[arxiv](https://arxiv.org/abs/2307.08771)] [[paper with code](https://paperswithcode.com/paper/upscale-unconstrained-channel-pruning)] [[code](https://github.com/apple/ml-upscale)]

- A Three-regime Model of Network Pruning [[paper](https://proceedings.mlr.press/v202/zhou23p.html)] [[arxiv](https://arxiv.org/abs/2305.18383)] [[paper with code](https://paperswithcode.com/paper/a-three-regime-model-of-network-pruning)] [[code](https://github.com/yefanzhou/threeregimepruning)]


## ACL-2023


- Pruning Pre-trained Language Models Without Fine-Tuning [[paper](https://aclanthology.org/2023.acl-long.35/)] [[arxiv](https://arxiv.org/abs/2210.06210)] [[paper with code](https://paperswithcode.com/paper/pruning-pre-trained-language-models-without)] [[code](https://github.com/kongds/smp)]

- Gradient-based Intra-attention Pruning on Pre-trained Language Models [[paper](https://aclanthology.org/2023.acl-long.156/)] [[arxiv](https://arxiv.org/abs/2212.07634)] [[paper with code](https://paperswithcode.com/paper/gradient-based-intra-attention-pruning-on-pre)] [[code](https://github.com/airaria/grain)]

- Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model [[paper](https://aclanthology.org/2023.acl-long.198/)] [[arxiv](https://arxiv.org/abs/2212.09811)] [[paper with code](https://paperswithcode.com/paper/memory-efficient-nllb-200-language-specific)]

- PuMer: Pruning and Merging Tokens for Efficient Vision Language Models [[paper](https://aclanthology.org/2023.acl-long.721/)] [[arxiv](https://arxiv.org/abs/2305.17530)] [[paper with code](https://paperswithcode.com/paper/pumer-pruning-and-merging-tokens-for)] [[code](https://github.com/csarron/pumer)]


## IJCAI-2023


- Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention [[paper](https://www.ijcai.org/proceedings/2023/136)] [[arxiv](https://arxiv.org/abs/2209.13802)] [[paper with code](https://paperswithcode.com/paper/adaptive-sparse-vit-towards-learnable)] [[code](https://github.com/cydia2018/as-vit)]

- Scaling Goal-based Exploration via Pruning Proto-goals [[paper](https://www.ijcai.org/proceedings/2023/384)] [[arxiv](https://arxiv.org/abs/2302.04693)] [[paper with code](https://paperswithcode.com/paper/scaling-goal-based-exploration-via-pruning)] [[code](https://github.com/facebookresearch/minihack)]

- Towards Lossless Head Pruning through Automatic Peer Distillation for Language Models [[paper](https://www.ijcai.org/proceedings/2023/568)]


## AAAI-2023


- Not All Neighbors Matter: Point Distribution-Aware Pruning for 3D Point Cloud [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25207)]

- Memory-Oriented Structural Pruning for Efficient Image Restoration [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25319)]

- Learning Second-Order Attentive Context for Efficient Correspondence Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25431)] [[arxiv](https://arxiv.org/abs/2303.15761)] [[paper with code](https://paperswithcode.com/paper/learning-second-order-attentive-context-for)]

- Efficient Distributed Inference of Deep Neural Networks via Restructuring and Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25815)]

- Complement Sparsification: Low-Overhead Model Pruning for Federated Learning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25977)] [[arxiv](https://arxiv.org/abs/2303.06237)] [[paper with code](https://paperswithcode.com/paper/complement-sparsification-low-overhead-model)]

- Balanced Column-Wise Block Pruning for Maximizing GPU Parallelism [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/26126)]

- Dynamic Structure Pruning for Compressing CNNs [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/26127)] [[arxiv](https://arxiv.org/abs/2303.09736)] [[paper with code](https://paperswithcode.com/paper/dynamic-structure-pruning-for-compressing)] [[code](https://github.com/irishev/dsp)]



# 2022


## NeurIPS-2022


- Robust Binary Models by Pruning Randomly-initialized Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/035f23c0ac4cf2b73b9365ba5a98ad56-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2202.01341)] [[paper with code](https://paperswithcode.com/paper/robust-binary-models-by-pruning-randomly)] [[code](https://github.com/IVRL/RobustBinarySubNet)] [[openreview](https://openreview.net/forum?id=5g-h_DILemH)]

- Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2208.11580)] [[paper with code](https://paperswithcode.com/paper/optimal-brain-compression-a-framework-for)] [[code](https://github.com/ist-daslab/obc)] [[openreview](https://openreview.net/forum?id=ksVGCOlOEba)]

- Spatial Pruned Sparse Convolution for Efficient 3D Object Detection [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/2ce10f144bb93449767f355c01f24cc1-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2209.14201)] [[paper with code](https://paperswithcode.com/paper/spatial-pruned-sparse-convolution-for)] [[openreview](https://openreview.net/forum?id=QqWqFLbllZh)]

- Structural Pruning via Latency-Saliency Knapsack [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/5434be94e82c54327bb9dcaf7fca52b6-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2210.06659)] [[paper with code](https://paperswithcode.com/paper/structural-pruning-via-latency-saliency)] [[code](https://github.com/NVlabs/HALP)] [[openreview](https://openreview.net/forum?id=cUOR-_VsavA)]

- Pruning has a disparate impact on model accuracy [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7087c949df293f13c0052ac825936e6f-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2205.13574)] [[paper with code](https://paperswithcode.com/paper/pruning-has-a-disparate-impact-on-model)] [[openreview](https://openreview.net/forum?id=11nMVZK0WYM)]

- Advancing Model Pruning via Bi-level Optimization [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/749252feedd44f7f10d47ec1d674a2f8-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2210.04092)] [[paper with code](https://paperswithcode.com/paper/advancing-model-pruning-via-bi-level)] [[code](https://github.com/optml-group/bip)] [[openreview](https://openreview.net/forum?id=t6O08FxvtBY)]

- Beyond neural scaling laws: beating power law scaling via data pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7b75da9b61eda40fa35453ee5d077df6-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2206.14486)] [[paper with code](https://paperswithcode.com/paper/beyond-neural-scaling-laws-beating-power-law)] [[code](https://github.com/rgeirhos/dataset-pruning-metrics)] [[openreview](https://openreview.net/forum?id=UmvSlP-PyV)]

- Sparse Probabilistic Circuits via Pruning and Growing [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b6089408f4893289296ad0499783b3a6-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2211.12551)] [[paper with code](https://paperswithcode.com/paper/sparse-probabilistic-circuits-via-pruning-and)] [[code](https://github.com/ucla-starai/sparsepc)] [[openreview](https://openreview.net/forum?id=KieCChVB6mN)]

- Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c2d82a425af4c18a35049899fea5ee82-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2205.13816)] [[paper with code](https://paperswithcode.com/paper/prune-and-distill-similar-reformatting-of)] [[openreview](https://openreview.net/forum?id=2OpRgzLhoPQ)]

- Recall Distortion in Neural Network Pruning and the Undecayed Pruning Algorithm [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/d3303e0ca98a267164d905bbc7947f88-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2206.02976)] [[paper with code](https://paperswithcode.com/paper/recall-distortion-in-neural-network-pruning)] [[openreview](https://openreview.net/forum?id=5hgYi4r5MDp)]

- Data-Efficient Structured Pruning via Submodular Optimization [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ed5854c456e136afa3faa5e41b1f3509-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2203.04940)] [[paper with code](https://paperswithcode.com/paper/data-efficient-structured-pruning-via)] [[code](https://github.com/marwash25/subpruning)] [[openreview](https://openreview.net/forum?id=K2QGzyLwpYG)]

- Pruning's Effect on Generalization Through the Lens of Training and Regularization [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/f7ede9414083fceab9e63d9100a80b36-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2210.13738)]

- Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/f7fc38fdd95fd146a471791b93ff9f12-Abstract-Conference.html)] [[arxiv](https://arxiv.org/abs/2209.08554)] [[paper with code](https://paperswithcode.com/paper/pruning-neural-networks-via-coresets-and)] [[openreview](https://openreview.net/forum?id=btpIaJiRx6z)]


## CVPR-2022


- Fire Together Wire Together: A Dynamic Pruning Approach With Self-Supervised Mask Prediction [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Elkerdawy_Fire_Together_Wire_Together_A_Dynamic_Pruning_Approach_With_Self-Supervised_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2110.08232)] [[paper with code](https://paperswithcode.com/paper/fire-together-wire-together-a-dynamic-pruning)] [[code](https://github.com/selkerdawy/FTWT)]

- Dreaming To Prune Image Deraining Networks [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zou_Dreaming_To_Prune_Image_Deraining_Networks_CVPR_2022_paper.html)] [[paper with code](https://paperswithcode.com/paper/dreaming-to-prune-image-deraining-networks)]

- Revisiting Random Channel Pruning for Neural Network Compression [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Revisiting_Random_Channel_Pruning_for_Neural_Network_Compression_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2205.05676)] [[paper with code](https://paperswithcode.com/paper/revisiting-random-channel-pruning-for-neural)] [[code](https://github.com/athulshibu/Rewarded-meta-pruning)]

- Interspace Pruning: Using Adaptive Filter Representations To Improve Training of Sparse CNNs [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Wimmer_Interspace_Pruning_Using_Adaptive_Filter_Representations_To_Improve_Training_of_CVPR_2022_paper.html)] [[arxiv](https://arxiv.org/abs/2203.07808)] [[paper with code](https://paperswithcode.com/paper/interspace-pruning-using-adaptive-filter)]

- When To Prune? A Policy Towards Early Structural Pruning [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Shen_When_To_Prune_A_Policy_Towards_Early_Structural_Pruning_CVPR_2022_paper.html)] [[paper with code](https://paperswithcode.com/paper/when-to-prune-a-policy-towards-early)]


## ICLR-2022


- Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions [[paper](https://iclr.cc/virtual/2022/poster/6611)] [[paper with code](https://paperswithcode.com/paper/revisit-kernel-pruning-with-lottery-regulated)] [[code](https://github.com/choH/lottery_regulated_grouped_kernel_pruning)] [[openreview](https://openreview.net/forum?id=LdEhiMG9WLO)]

- The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training [[paper](https://iclr.cc/virtual/2022/poster/6925)] [[arxiv](https://arxiv.org/abs/2202.02643)] [[paper with code](https://paperswithcode.com/paper/the-unreasonable-effectiveness-of-random-1)] [[code](https://github.com/vita-group/random_pruning)] [[openreview](https://openreview.net/forum?id=VBZJ_3tz-t)]

- SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning [[paper](https://iclr.cc/virtual/2022/poster/7189)] [[arxiv](https://arxiv.org/abs/2110.11395)] [[paper with code](https://paperswithcode.com/paper/sosp-efficiently-capturing-global-1)] [[code](https://github.com/boschresearch/sosp)] [[openreview](https://openreview.net/forum?id=sUgpxb9QD)]

- Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients [[paper](https://iclr.cc/virtual/2022/poster/6758)] [[arxiv](https://arxiv.org/abs/2202.08132)] [[paper with code](https://paperswithcode.com/paper/prospect-pruning-finding-trainable-weights-at-1)] [[code](https://github.com/mil-ad/prospr)] [[openreview](https://openreview.net/forum?id=AIgn9uwfcD1)]

- An Operator Theoretic View On Pruning Deep Neural Networks [[paper](https://iclr.cc/virtual/2022/poster/6940)] [[arxiv](https://arxiv.org/abs/2110.14856)] [[paper with code](https://paperswithcode.com/paper/an-operator-theoretic-perspective-on-pruning)] [[openreview](https://openreview.net/forum?id=pWBNOgdeURp)]

- Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning [[paper](https://iclr.cc/virtual/2022/poster/5963)] [[paper with code](https://paperswithcode.com/paper/learning-efficient-image-super-resolution)] [[openreview](https://openreview.net/forum?id=AjGC97Aofee)]

- Effective Model Sparsification by Scheduled Grow-and-Prune Methods [[paper](https://iclr.cc/virtual/2022/poster/6315)] [[arxiv](https://arxiv.org/abs/2106.09857)] [[paper with code](https://paperswithcode.com/paper/effective-model-sparsification-by-scheduled)] [[code](https://github.com/boone891214/gap)] [[openreview](https://openreview.net/forum?id=xa6otUDdP2W)]

- Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining [[paper](https://iclr.cc/virtual/2022/poster/6158)] [[paper with code](https://paperswithcode.com/paper/learning-pruning-friendly-networks-via-frank)] [[code](https://github.com/VITA-Group/SFW-Once-for-All-Pruning)] [[openreview](https://openreview.net/forum?id=O1DEtITim__)]


## ECCV-2022


- Prune Your Model before Distill It [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1136_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2109.14960)] [[paper with code](https://paperswithcode.com/paper/prune-your-model-before-distill-it)] [[code](https://github.com/ososos888/prune-then-distill)]

- Disentangled Differentiable Network Pruning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4273_ECCV_2022_paper.php)]

- Multi-Granularity Pruning for Model Acceleration on Mobile Devices [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5496_ECCV_2022_paper.php)]

- Ensemble Knowledge Guided Sub-network Search and Fine-Tuning for Filter Pruning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6024_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2203.02651)] [[paper with code](https://paperswithcode.com/paper/ensemble-knowledge-guided-sub-network-search)] [[code](https://github.com/sseung0703/ekg)]

- Soft Masking for Cost-Constrained Channel Pruning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6269_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2211.02206)] [[paper with code](https://paperswithcode.com/paper/soft-masking-for-cost-constrained-channel)] [[code](https://github.com/nvlabs/smcp)]

- SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6312_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2207.03677)] [[paper with code](https://paperswithcode.com/paper/supertickets-drawing-task-agnostic-lottery)] [[code](https://github.com/rice-eic/supertickets)]

- Towards Ultra Low Latency Spiking Neural Networks for Vision and Sequential Tasks Using Temporal Pruning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6421_ECCV_2022_paper.php)]

- FairGRAPE: Fairness-Aware GRAdient Pruning mEthod for Face Attribute Classification [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4688_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2207.10888)] [[paper with code](https://paperswithcode.com/paper/fairgrape-fairness-aware-gradient-pruning)] [[code](https://github.com/bernardo1998/fairgrape)]

- CPrune: Compiler-Informed Model Pruning for Efficient Target-Aware DNN Execution [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2171_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2207.01260)] [[paper with code](https://paperswithcode.com/paper/cprune-compiler-informed-model-pruning-for)] [[code](https://github.com/taehokim20/cprune)]

- Filter Pruning via Feature Discrimination in Deep Neural Networks [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6835_ECCV_2022_paper.php)]

- Interpretations Steered Network Pruning via Amortized Inferred Saliency Maps [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7153_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2209.02869)] [[paper with code](https://paperswithcode.com/paper/interpretations-steered-network-pruning-via)] [[code](https://github.com/Alii-Ganjj/InterpretationsSteeredPruning)]

- Bayesian Optimization with Clustering and Rollback for CNN Auto Pruning [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4872_ECCV_2022_paper.php)] [[arxiv](https://arxiv.org/abs/2109.10591)] [[paper with code](https://paperswithcode.com/paper/high-dimensional-bayesian-optimization-for)] [[code](https://github.com/fanhanwei/bocr)]


## ICML-2022


- Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness [[paper](https://proceedings.mlr.press/v162/chen22af.html)] [[arxiv](https://arxiv.org/abs/2206.07839)] [[paper with code](https://paperswithcode.com/paper/linearity-grafting-relaxed-neuron-pruning)] [[code](https://github.com/vita-group/linearity-grafting)]

- SPDY: Accurate Pruning with Speedup Guarantees [[paper](https://proceedings.mlr.press/v162/frantar22a.html)] [[arxiv](https://arxiv.org/abs/2201.13096)] [[paper with code](https://paperswithcode.com/paper/spdy-accurate-pruning-with-speedup-guarantees)] [[code](https://github.com/ist-daslab/spdy)]

- Sparse Double Descent: Where Network Pruning Aggravates Overfitting [[paper](https://proceedings.mlr.press/v162/he22d.html)] [[arxiv](https://arxiv.org/abs/2206.08684)] [[paper with code](https://paperswithcode.com/paper/sparse-double-descent-where-network-pruning)] [[code](https://github.com/hezheug/sparse-double-descent)]

- PAC-Net: A Model Pruning Approach to Inductive Transfer Learning [[paper](https://proceedings.mlr.press/v162/myung22a.html)] [[arxiv](https://arxiv.org/abs/2206.05703)] [[paper with code](https://paperswithcode.com/paper/pac-net-a-model-pruning-approach-to-inductive)]

- Neural Network Pruning Denoises the Features and Makes Local Connectivity Emerge in Visual Tasks [[paper](https://proceedings.mlr.press/v162/pellegrini22a.html)]

- Winning the Lottery Ahead of Time: Efficient Early Network Pruning [[paper](https://proceedings.mlr.press/v162/rachwan22a.html)] [[arxiv](https://arxiv.org/abs/2206.10451)] [[paper with code](https://paperswithcode.com/paper/winning-the-lottery-ahead-of-time-efficient)] [[code](https://github.com/johnrachwan123/Early-Cropression-via-Gradient-Flow-Preservation)]

- Topology-Aware Network Pruning using Multi-stage Graph Embedding and Reinforcement Learning [[paper](https://proceedings.mlr.press/v162/yu22e.html)] [[arxiv](https://arxiv.org/abs/2102.03214)] [[paper with code](https://paperswithcode.com/paper/gnn-rl-compression-topology-aware-network)] [[code](https://github.com/yusx-swapp/gnn-rl-model-compression)]

- The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks [[paper](https://proceedings.mlr.press/v162/yu22f.html)] [[arxiv](https://arxiv.org/abs/2203.04466)] [[paper with code](https://paperswithcode.com/paper/the-combinatorial-brain-surgeon-pruning)] [[code](https://github.com/yuxwind/cbs)]


## ACL-2022


- Structured Pruning Learns Compact and Accurate Models [[paper](https://aclanthology.org/2022.acl-long.107/)] [[arxiv](https://arxiv.org/abs/2204.00408)] [[paper with code](https://paperswithcode.com/paper/structured-pruning-learns-compact-and-1)] [[code](https://github.com/princeton-nlp/cofipruning)]

- Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency [[paper](https://aclanthology.org/2022.acl-long.130/)] [[arxiv](https://arxiv.org/abs/2204.02601)] [[paper with code](https://paperswithcode.com/paper/probing-structured-pruning-on-multilingual)]


## IJCAI-2022


- FedDUAP: Federated Learning with Dynamic Update and Adaptive Pruning Using Shared Data on the Server [[paper](https://www.ijcai.org/proceedings/2022/385)] [[arxiv](https://arxiv.org/abs/2204.11536)] [[paper with code](https://paperswithcode.com/paper/fedduap-federated-learning-with-dynamic)]

- On the Channel Pruning using Graph Convolution Network for Convolutional Neural Network Acceleration [[paper](https://www.ijcai.org/proceedings/2022/431)]

- Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization [[paper](https://www.ijcai.org/proceedings/2022/449)] [[arxiv](https://arxiv.org/abs/2206.01198)] [[paper with code](https://paperswithcode.com/paper/pruning-as-search-efficient-neural)] [[code](https://github.com/liyy201912/PaS)]

- Model-Based Offline Planning with Trajectory Pruning [[paper](https://www.ijcai.org/proceedings/2022/516)] [[arxiv](https://arxiv.org/abs/2105.07351)] [[paper with code](https://paperswithcode.com/paper/model-based-offline-planning-with-trajectory)] [[code](https://github.com/zhanzxy5/MOPP)]

- Neural Subgraph Explorer: Reducing Noisy Information via Target-oriented Syntax Graph Pruning [[paper](https://www.ijcai.org/proceedings/2022/614)] [[arxiv](https://arxiv.org/abs/2205.10970)] [[paper with code](https://paperswithcode.com/paper/neural-subgraph-explorer-reducing-noisy)]

- Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph [[paper](https://www.ijcai.org/proceedings/2022/632)] [[arxiv](https://arxiv.org/abs/2112.06013)] [[paper with code](https://paperswithcode.com/paper/efficient-document-level-event-extraction-via)] [[code](https://github.com/Spico197/DocEE)]

- Neural Network Pruning by Cooperative Coevolution [[paper](https://www.ijcai.org/proceedings/2022/667)] [[arxiv](https://arxiv.org/abs/2204.05639)] [[paper with code](https://paperswithcode.com/paper/neural-network-pruning-by-cooperative)]

- Recent Advances on Neural Network Pruning at Initialization [[paper](https://www.ijcai.org/proceedings/2022/786)] [[arxiv](https://arxiv.org/abs/2103.06460)] [[paper with code](https://paperswithcode.com/paper/emerging-paradigms-of-neural-network-pruning)] [[code](https://github.com/mingsun-tse/awesome-pruning-at-initialization)]


## AAAI-2022


- Prior Gradient Mask Guided Pruning-Aware Fine-Tuning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/19888)] [[paper with code](https://paperswithcode.com/paper/prior-gradient-mask-guided-pruning-aware-fine)] [[code](https://github.com/cailinhang/PGMPF)]

- Prune and Tune Ensembles: Low-Cost Ensemble Learning with Sparse Independent Subnetworks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20842)] [[arxiv](https://arxiv.org/abs/2202.11782)] [[paper with code](https://paperswithcode.com/paper/prune-and-tune-ensembles-low-cost-ensemble)] [[code](https://github.com/Claydon-Wang/Anti-random-pruning)]

- Span-Based Semantic Role Labeling with Argument Pruning and Second-Order Inference [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/21328)]

- From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/21408)] [[arxiv](https://arxiv.org/abs/2112.07198)] [[paper with code](https://paperswithcode.com/paper/from-dense-to-sparse-contrastive-pruning-for)] [[code](https://github.com/alibaba/AliceMind)]



# 2021


## NeurIPS-2021


- Pruning Randomly Initialized Neural Networks with Iterative Randomization [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/23e582ad8087f2c03a5a31c125123f9a-Abstract.html)] [[arxiv](https://arxiv.org/abs/2106.09269)] [[paper with code](https://paperswithcode.com/paper/pruning-randomly-initialized-neural-networks)] [[code](https://github.com/dchiji-ntt/iterand)] [[openreview](https://openreview.net/forum?id=QCPY2eMXYs)]

- Sparse Training via Boosting Pruning Plasticity with Neuroregeneration [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/5227b6aaf294f5f027273aebf16015f2-Abstract.html)] [[arxiv](https://arxiv.org/abs/2106.10404)]

- Rethinking the Pruning Criteria for Convolutional Neural Network [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/87ae6fb631f7c8a627e8e28785d9992d-Abstract.html)] [[paper with code](https://paperswithcode.com/paper/rethinking-the-pruning-criteria-for)] [[openreview](https://openreview.net/forum?id=HL_4vjPTdtp)]

- Adversarial Neuron Pruning Purifies Backdoored Deep Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/8cbe9ce23f42628c98f80fa0fac8b19a-Abstract.html)] [[arxiv](https://arxiv.org/abs/2110.14430)] [[paper with code](https://paperswithcode.com/paper/adversarial-neuron-pruning-purifies)] [[code](https://github.com/csdongxian/anp_backdoor)] [[openreview](https://openreview.net/forum?id=4cEapqXfP30)]

- Only Train Once: A One-Shot Neural Network Training And Pruning Framework [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/a376033f78e144f494bfc743c0be3330-Abstract.html)] [[arxiv](https://arxiv.org/abs/2107.07467)] [[paper with code](https://paperswithcode.com/paper/only-train-once-a-one-shot-neural-network)] [[code](https://github.com/tianyic/only_train_once)] [[openreview](https://openreview.net/forum?id=p5rMPjrcCZq)]

- PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/b17c0907e67d868b4e0feb43dbbe6f11-Abstract.html)] [[arxiv](https://arxiv.org/abs/2106.05933)] [[paper with code](https://paperswithcode.com/paper/parp-prune-adjust-and-re-prune-for-self)] [[openreview](https://openreview.net/forum?id=UoVpP8R2Vn)]

- Sparse Flows: Pruning Continuous-depth Models [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/bf1b2f4b901c21a1d8645018ea9aeb05-Abstract.html)] [[arxiv](https://arxiv.org/abs/2106.12718)] [[paper with code](https://paperswithcode.com/paper/sparse-flows-pruning-continuous-depth-models)] [[code](https://github.com/lucaslie/torchprune)] [[openreview](https://openreview.net/forum?id=_1VZo_-aUiT)]

- CHIP: CHannel Independence-based Pruning for Compact Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/ce6babd060aa46c61a5777902cca78af-Abstract.html)] [[arxiv](https://arxiv.org/abs/2110.13981)] [[paper with code](https://paperswithcode.com/paper/chip-channel-independence-based-pruning-for)] [[code](https://github.com/eclipsess/chip_neurips2021)]


## CVPR-2021


- Learnable Motion Coherence for Correspondence Pruning [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Learnable_Motion_Coherence_for_Correspondence_Pruning_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2011.14563)] [[paper with code](https://paperswithcode.com/paper/learnable-motion-coherence-for-correspondence)]

- NPAS: A Compiler-Aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Li_NPAS_A_Compiler-Aware_Framework_of_Unified_Network_Pruning_and_Architecture_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2012.00596)] [[paper with code](https://paperswithcode.com/paper/6-7ms-on-mobile-with-over-78-imagenet)]

- Network Pruning via Performance Maximization [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Network_Pruning_via_Performance_Maximization_CVPR_2021_paper.html)] [[paper with code](https://paperswithcode.com/paper/network-pruning-via-performance-maximization)] [[code](https://github.com/gaosh/NPPM)]

- Convolutional Neural Network Pruning With Structural Redundancy Reduction [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Convolutional_Neural_Network_Pruning_With_Structural_Redundancy_Reduction_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2104.03438)] [[paper with code](https://paperswithcode.com/paper/convolutional-neural-network-pruning-with)]

- Manifold Regularized Dynamic Network Pruning [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_Manifold_Regularized_Dynamic_Network_Pruning_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2103.05861)] [[paper with code](https://paperswithcode.com/paper/manifold-regularized-dynamic-network-pruning)] [[code](https://github.com/mindspore-ai/models/tree/master/research/cv/ManiDP)]

- Joint-DetNAS: Upgrade Your Detector With NAS, Pruning and Dynamic Distillation [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Yao_Joint-DetNAS_Upgrade_Your_Detector_With_NAS_Pruning_and_Dynamic_Distillation_CVPR_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2105.12971)] [[paper with code](https://paperswithcode.com/paper/joint-detnas-upgrade-your-detector-with-nas)]


## ICLR-2021


- Pruning Neural Networks at Initialization: Why Are We Missing the Mark? [[paper](https://iclr.cc/virtual/2021/poster/3159)] [[paper with code](https://paperswithcode.com/paper/pruning-neural-networks-at-initialization-why)] [[openreview](https://openreview.net/forum?id=Ig-VyQc-MLK)]

- Layer-adaptive Sparsity for the Magnitude-based Pruning [[paper](https://iclr.cc/virtual/2021/poster/3108)] [[arxiv](https://arxiv.org/abs/2010.07611)] [[paper with code](https://paperswithcode.com/paper/a-deeper-look-at-the-layerwise-sparsity-of-1)] [[code](https://github.com/jaeho-lee/layer-adaptive-sparsity)] [[openreview](https://openreview.net/forum?id=H6ATjJ0TKdf)]

- Robust Pruning at Initialization [[paper](https://iclr.cc/virtual/2021/poster/2902)] [[arxiv](https://arxiv.org/abs/2002.08797)]

- A Gradient Flow Framework For Analyzing Network Pruning [[paper](https://iclr.cc/virtual/2021/poster/3215)] [[arxiv](https://arxiv.org/abs/2009.11839)] [[paper with code](https://paperswithcode.com/paper/a-gradient-flow-framework-for-analyzing)] [[code](https://github.com/EkdeepSLubana/flowandprune)] [[openreview](https://openreview.net/forum?id=rumv7QmLUue)]

- Network Pruning That Matters: A Case Study on Retraining Variants [[paper](https://iclr.cc/virtual/2021/poster/2991)] [[arxiv](https://arxiv.org/abs/2105.03193)]

- ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations [[paper](https://iclr.cc/virtual/2021/poster/2768)] [[arxiv](https://arxiv.org/abs/2102.07156)] [[paper with code](https://paperswithcode.com/paper/chipnet-budget-aware-pruning-with-heaviside-1)] [[code](https://github.com/transmuteAI/ChipNet)] [[openreview](https://openreview.net/forum?id=xCxXwTzx4L1)]

- Neural Pruning via Growing Regularization [[paper](https://iclr.cc/virtual/2021/poster/2526)] [[arxiv](https://arxiv.org/abs/2012.09243)] [[paper with code](https://paperswithcode.com/paper/neural-pruning-via-growing-regularization-1)] [[code](https://github.com/mingsun-tse/regularization-pruning)] [[openreview](https://openreview.net/forum?id=o966_Is_nPA)]

- Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network [[paper](https://iclr.cc/virtual/2021/poster/3050)] [[arxiv](https://arxiv.org/abs/2103.09377)] [[paper with code](https://paperswithcode.com/paper/multi-prize-lottery-ticket-hypothesis-finding-1)] [[code](https://github.com/chrundle/biprop)]


## ICCV-2021


- ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Ding_ResRep_Lossless_CNN_Pruning_via_Decoupling_Remembering_and_Forgetting_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2007.03260)] [[paper with code](https://paperswithcode.com/paper/lossless-cnn-channel-pruning-via-gradient)] [[code](https://github.com/DingXiaoH/ResRep)]

- Achieving On-Mobile Real-Time Super-Resolution With Neural Architecture and Pruning Search [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zhan_Achieving_On-Mobile_Real-Time_Super-Resolution_With_Neural_Architecture_and_Pruning_Search_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2108.08910)] [[paper with code](https://paperswithcode.com/paper/achieving-on-mobile-real-time-super)]

- Progressive Correspondence Pruning by Consensus Learning [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Progressive_Correspondence_Pruning_by_Consensus_Learning_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2101.00591)] [[paper with code](https://paperswithcode.com/paper/consensus-guided-correspondence-denoising)] [[code](https://github.com/weitong8591/ars_magsac)]

- GDP: Stabilized Neural Network Pruning via Gates With Differentiable Polarization [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Guo_GDP_Stabilized_Neural_Network_Pruning_via_Gates_With_Differentiable_Polarization_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2109.02220)] [[paper with code](https://paperswithcode.com/paper/gdp-stabilized-neural-network-pruning-via)]

- Auto Graph Encoder-Decoder for Neural Network Pruning [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Yu_Auto_Graph_Encoder-Decoder_for_Neural_Network_Pruning_ICCV_2021_paper.html)] [[arxiv](https://arxiv.org/abs/2011.12641)] [[paper with code](https://paperswithcode.com/paper/auto-graph-encoder-decoder-for-model)]


## ICML-2021


- Group Fisher Pruning for Practical Network Compression [[paper](https://proceedings.mlr.press/v139/liu21ab.html)] [[arxiv](https://arxiv.org/abs/2108.00708)] [[paper with code](https://paperswithcode.com/paper/group-fisher-pruning-for-practical-network)] [[code](https://github.com/jshilong/FisherPruning)]

- A Probabilistic Approach to Neural Network Pruning [[paper](https://proceedings.mlr.press/v139/qian21a.html)] [[arxiv](https://arxiv.org/abs/2105.10065)] [[paper with code](https://paperswithcode.com/paper/a-probabilistic-approach-to-neural-network)]

- On the Predictability of Pruning Across Scales [[paper](https://proceedings.mlr.press/v139/rosenfeld21a.html)] [[arxiv](https://arxiv.org/abs/2006.10621)] [[paper with code](https://paperswithcode.com/paper/on-the-predictability-of-pruning-across)]

- Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework [[paper](https://proceedings.mlr.press/v139/wang21e.html)] [[arxiv](https://arxiv.org/abs/2010.04879)] [[paper with code](https://paperswithcode.com/paper/accelerate-your-cnn-from-three-dimensions-a)]


## ACL-2021


- Parameter-Efficient Transfer Learning with Diff Pruning [[paper](https://aclanthology.org/2021.acl-long.378/)] [[arxiv](https://arxiv.org/abs/2012.07463)] [[paper with code](https://paperswithcode.com/paper/parameter-efficient-transfer-learning-with-1)] [[code](https://github.com/dguo98/DiffPruning)]

- Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking [[paper](https://aclanthology.org/2021.acl-short.66/)] [[arxiv](https://arxiv.org/abs/2107.08173)] [[paper with code](https://paperswithcode.com/paper/continual-learning-for-task-oriented-dialogue)] [[code](https://github.com/siat-nlp/TPEM)]


## IJCAI-2021


- Pruning of Deep Spiking Neural Networks through Gradient Rewiring [[paper](https://www.ijcai.org/proceedings/2021/236)] [[arxiv](https://arxiv.org/abs/2105.04916)] [[paper with code](https://paperswithcode.com/paper/pruning-of-deep-spiking-neural-networks)] [[code](https://github.com/Yanqi-Chen/Gradient-Rewiring)]

- Enabling Retrain-free Deep Neural Network Pruning Using Surrogate Lagrangian Relaxation [[paper](https://www.ijcai.org/proceedings/2021/344)] [[arxiv](https://arxiv.org/abs/2012.10079)] [[paper with code](https://paperswithcode.com/paper/a-surrogate-lagrangian-relaxation-based-model)]

- Against Membership Inference Attack: Pruning is All You Need [[paper](https://www.ijcai.org/proceedings/2021/432)] [[arxiv](https://arxiv.org/abs/2008.13578)] [[paper with code](https://paperswithcode.com/paper/mcmia-model-compression-against-membership)]

- The Fewer the Merrier: Pruning Preferred Operators with Novelty [[paper](https://www.ijcai.org/proceedings/2021/576)]


## AAAI-2021


- AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in Fine-tuning of Deep Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16350)] [[arxiv](https://arxiv.org/abs/2002.06048)] [[paper with code](https://paperswithcode.com/paper/layer-wise-pruning-and-auto-tuning-of-layer)] [[code](https://github.com/youngminPIL/AutoLR)]

- DPFPS: Dynamic and Progressive Filter Pruning for Compressing Convolutional Neural Networks from Scratch [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16351)]

- Provable Benefits of Overparameterization in Model Compression: From Double Descent to Pruning Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16859)] [[arxiv](https://arxiv.org/abs/2012.08749)] [[paper with code](https://paperswithcode.com/paper/provable-benefits-of-overparameterization-in)]

- OPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16950)] [[arxiv](https://arxiv.org/abs/2205.11141)] [[paper with code](https://paperswithcode.com/paper/opq-compressing-deep-neural-networks-with-one)]

- Linearly Replaceable Filters for Deep Network Channel Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16978)]

- TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17046)] [[arxiv](https://arxiv.org/abs/2103.01542)] [[paper with code](https://paperswithcode.com/paper/transtailor-pruning-the-pre-trained-model-for)]

- Revisiting Dominance Pruning in Decoupled Search [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17403)]

- On the Optimal Efficiency of A* with Dominance Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17426)]


# 2020


## NeurIPS-2020


- Neuron Merging: Compensating for Pruned Neurons [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/0678ca2eae02d542cc931e81b74de122-Abstract.html)] [[arxiv](https://arxiv.org/abs/2010.13160)] [[paper with code](https://paperswithcode.com/paper/neuron-merging-compensating-for-pruned)] [[code](https://github.com/friendshipkim/neuron-merging)]

- Logarithmic Pruning is All You Need [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1e9491470749d5b0e361ce4f0b24d037-Abstract.html)] [[arxiv](https://arxiv.org/abs/2006.12156)] [[paper with code](https://paperswithcode.com/paper/logarithmic-pruning-is-all-you-need)]

- Bayesian Bits: Unifying Quantization and Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Abstract.html)] [[arxiv](https://arxiv.org/abs/2005.07093)] [[paper with code](https://paperswithcode.com/paper/bayesian-bits-unifying-quantization-and)] [[code](https://github.com/Qualcomm-AI-research/BayesianBits)]

- Pruning neural networks without any data by iteratively conserving synaptic flow [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html)] [[arxiv](https://arxiv.org/abs/2006.05467)] [[paper with code](https://paperswithcode.com/paper/pruning-neural-networks-without-any-data-by)] [[code](https://github.com/ganguli-lab/Synaptic-Flow)]

- Neuron-level Structured Pruning using Polarization Regularizer [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/703957b6dd9e3a7980e040bee50ded65-Abstract.html)] [[paper with code](https://paperswithcode.com/paper/neuron-level-structured-pruning-using)] [[code](https://github.com/polarizationpruning/PolarizationPruning)]

- SCOP: Scientific Control for Reliable Neural Network Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html)] [[arxiv](https://arxiv.org/abs/2010.10732)] [[paper with code](https://paperswithcode.com/paper/scop-scientific-control-for-reliable-neural)] [[code](https://github.com/huawei-noah/Pruning)]

- Directional Pruning of Deep Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/a09e75c5c86a7bf6582d2b4d75aad615-Abstract.html)] [[arxiv](https://arxiv.org/abs/2006.09358)] [[paper with code](https://paperswithcode.com/paper/directional-pruning-of-deep-neural-networks)] [[code](https://github.com/donlan2710/gRDA-Optimizer)]

- Using noise to probe recurrent neural network structure and prune synapses [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/a1ada9947e0d683b4625f94c74104d73-Abstract.html)] [[arxiv](https://arxiv.org/abs/2011.07334)] [[paper with code](https://paperswithcode.com/paper/using-noise-to-probe-recurrent-neural-network-1)]

- Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/a914ecef9c12ffdb9bede64bb703d877-Abstract.html)] [[paper with code](https://paperswithcode.com/paper/storage-efficient-and-dynamic-flexible)] [[openreview](https://openreview.net/forum?id=S1ewjhEFwr)]

- Pruning Filter in Filter [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/ccb1d45fb76f7c5a0bf619f979c6cf36-Abstract.html)] [[arxiv](https://arxiv.org/abs/2009.14410)] [[paper with code](https://paperswithcode.com/paper/pruning-filter-in-filter)] [[code](https://github.com/fxmeng/Pruning-Filter-in-Filter)]

- HYDRA: Pruning Adversarially Robust Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/e3a72c791a69f87b05ea7742e04430ed-Abstract.html)] [[arxiv](https://arxiv.org/abs/2002.10509)] [[paper with code](https://paperswithcode.com/paper/on-pruning-adversarially-robust-neural)] [[code](https://github.com/inspire-group/compactness-robustness)]

- Movement Pruning: Adaptive Sparsity by Fine-Tuning [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/eae15aabaa768ae4a5993a8a4f4fa6e4-Abstract.html)] [[arxiv](https://arxiv.org/abs/2005.07683)] [[paper with code](https://paperswithcode.com/paper/movement-pruning-adaptive-sparsity-by-fine)] [[code](https://github.com/huggingface/transformers/tree/master/examples/movement-pruning)]

- Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html)] [[arxiv](https://arxiv.org/abs/2009.11094)] [[paper with code](https://paperswithcode.com/paper/sanity-checking-pruning-methods-random)] [[code](https://github.com/JingtongSu/sanity-checking-pruning)]

- Position-based Scaled Gradient for Model Quantization and Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html)] [[arxiv](https://arxiv.org/abs/2005.11035)] [[paper with code](https://paperswithcode.com/paper/position-based-scaled-gradient-for-model)] [[code](https://github.com/Jangho-Kim/PSG-pytorch)]

- The Generalization-Stability Tradeoff In Neural Network Pruning [[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef2ee09ea9551de88bc11fd7eeea93b0-Abstract.html)] [[arxiv](https://arxiv.org/abs/1906.03728)] [[paper with code](https://paperswithcode.com/paper/the-generalization-stability-tradeoff-in)]


## CVPR-2020


- Structured Compression by Weight Encryption for Unstructured Pruning and Quantization [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Kwon_Structured_Compression_by_Weight_Encryption_for_Unstructured_Pruning_and_Quantization_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/1905.10138)] [[paper with code](https://paperswithcode.com/paper/structured-compression-by-unstructured)]

- HRank: Filter Pruning Using High-Rank Feature Map [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Lin_HRank_Filter_Pruning_Using_High-Rank_Feature_Map_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/2002.10179)] [[paper with code](https://paperswithcode.com/paper/hrank-filter-pruning-using-high-rank-feature)] [[code](https://github.com/lmbxmu/HRank)]

- Multi-Dimensional Pruning: A Unified Framework for Model Compression [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_Multi-Dimensional_Pruning_A_Unified_Framework_for_Model_Compression_CVPR_2020_paper.html)] [[paper with code](https://paperswithcode.com/paper/multi-dimensional-pruning-a-unified-framework)]

- APQ: Joint Search for Network Architecture, Pruning and Quantization Policy [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_APQ_Joint_Search_for_Network_Architecture_Pruning_and_Quantization_Policy_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/2006.08509)] [[paper with code](https://paperswithcode.com/paper/apq-joint-search-for-network-architecture-1)] [[code](https://github.com/mit-han-lab/apq)]

- Neural Network Pruning With Residual-Connections and Limited-Data [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Luo_Neural_Network_Pruning_With_Residual-Connections_and_Limited-Data_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/1911.08114)] [[paper with code](https://paperswithcode.com/paper/neural-network-pruning-with-residual)] [[code](https://github.com/Roll920/CURL)]

- DMCP: Differentiable Markov Channel Pruning for Neural Networks [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_DMCP_Differentiable_Markov_Channel_Pruning_for_Neural_Networks_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/2005.03354)] [[paper with code](https://paperswithcode.com/paper/dmcp-differentiable-markov-channel-pruning)] [[code](https://github.com/Zx55/dmcp)]

- Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/He_Learning_Filter_Pruning_Criteria_for_Deep_Convolutional_Neural_Networks_Acceleration_CVPR_2020_paper.html)] [[paper with code](https://paperswithcode.com/paper/learning-filter-pruning-criteria-for-deep)]

- Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression [[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Group_Sparsity_The_Hinge_Between_Filter_Pruning_and_Decomposition_for_CVPR_2020_paper.html)] [[arxiv](https://arxiv.org/abs/2003.08935)] [[paper with code](https://paperswithcode.com/paper/group-sparsity-the-hinge-between-filter)] [[code](https://github.com/ofsoundof/group_sparsity)]


## ICLR-2020


- Comparing Rewinding and Fine-tuning in Neural Network Pruning [[paper](https://iclr.cc/virtual/2020/poster/1969)] [[arxiv](https://arxiv.org/abs/2003.02389)] [[paper with code](https://paperswithcode.com/paper/comparing-rewinding-and-fine-tuning-in-neural)] [[code](https://github.com/lottery-ticket/rewinding-iclr20-public)] [[openreview](https://openreview.net/forum?id=S1gSj0NKvB)]

- Data-Independent Neural Pruning via Coresets [[paper](https://iclr.cc/virtual/2020/poster/1817)] [[arxiv](https://arxiv.org/abs/1907.04018)] [[paper with code](https://paperswithcode.com/paper/on-activation-function-coresets-for-network)] [[openreview](https://openreview.net/forum?id=H1gmHaEKwB)]

- One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation [[paper](https://iclr.cc/virtual/2020/poster/1554)] [[arxiv](https://arxiv.org/abs/1912.00120)] [[paper with code](https://paperswithcode.com/paper/one-shot-pruning-of-recurrent-neural-networks-1)] [[openreview](https://openreview.net/forum?id=r1e9GCNKvH)]

- Provable Filter Pruning for Efficient Neural Networks [[paper](https://iclr.cc/virtual/2020/poster/2051)] [[arxiv](https://arxiv.org/abs/1911.07412)] [[paper with code](https://paperswithcode.com/paper/provable-filter-pruning-for-efficient-neural-1)] [[code](https://github.com/lucaslie/provable_pruning)] [[openreview](https://openreview.net/forum?id=BJxkOlSYDH)]

- A Signal Propagation Perspective for Pruning Neural Networks at Initialization [[paper](https://iclr.cc/virtual/2020/poster/1807)] [[arxiv](https://arxiv.org/abs/1906.06307)] [[paper with code](https://paperswithcode.com/paper/a-signal-propagation-perspective-for-pruning)] [[code](https://github.com/namhoonlee/spp-public)] [[openreview](https://openreview.net/forum?id=HJeTo2VFwH)]

- Dynamic Model Pruning with Feedback [[paper](https://iclr.cc/virtual/2020/poster/1702)] [[arxiv](https://arxiv.org/abs/2006.07253)] [[paper with code](https://paperswithcode.com/paper/dynamic-model-pruning-with-feedback-1)] [[openreview](https://openreview.net/forum?id=SJem8lSFwB)]

- Lookahead: A Far-sighted Alternative of Magnitude-based Pruning [[paper](https://iclr.cc/virtual/2020/poster/1943)] [[arxiv](https://arxiv.org/abs/2002.04809)] [[paper with code](https://paperswithcode.com/paper/lookahead-a-far-sighted-alternative-of-1)] [[code](https://github.com/alinlab/lookahead_pruning)] [[openreview](https://openreview.net/forum?id=ryl3ygHYDB)]

- Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning [[paper](https://iclr.cc/virtual/2020/poster/2081)] [[arxiv](https://arxiv.org/abs/1909.11334)] [[paper with code](https://paperswithcode.com/paper/dynamically-pruned-message-passing-networks)] [[code](https://github.com/netpaladinx/DPMPN)] [[openreview](https://openreview.net/forum?id=rkeuAhVKvB)]

- Pruned Graph Scattering Transforms [[paper](https://iclr.cc/virtual/2020/poster/1579)] [[paper with code](https://paperswithcode.com/paper/pruned-graph-scattering-transforms)] [[openreview](https://openreview.net/forum?id=rJeg7TEYwB)]


## ECCV-2020


- EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3948_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2007.02491)]

- DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/922_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2004.02164)]

- DHP: Differentiable Meta Pruning via HyperNetworks [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/637_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2003.13683)]

- Meta-Learning with Network Pruning [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3318_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2007.03219)]

- Accelerating CNN Training by Pruning Activation Gradients [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5004_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/1908.00173)]

- DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5875_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2003.12563)]

- Differentiable Joint Pruning and Quantization for Hardware Efficiency [[paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6693_ECCV_2020_paper.php)] [[arxiv](https://arxiv.org/abs/2007.10463)]


## ICML-2020


- Operation-Aware Soft Channel Pruning using Differentiable Masks [[paper](https://proceedings.mlr.press/v119/kang20a.html)] [[arxiv](https://arxiv.org/abs/2007.03938)] [[paper with code](https://paperswithcode.com/paper/operation-aware-soft-channel-pruning-using)] [[code](https://github.com/kminsoo/SCP)]

- PENNI: Pruned Kernel Sharing for Efficient CNN Inference [[paper](https://proceedings.mlr.press/v119/li20d.html)] [[arxiv](https://arxiv.org/abs/2005.07133)] [[paper with code](https://paperswithcode.com/paper/penni-pruned-kernel-sharing-for-efficient-cnn)] [[code](https://github.com/timlee0212/PENNI)]

- Adversarial Neural Pruning with Latent Vulnerability Suppression [[paper](https://proceedings.mlr.press/v119/madaan20a.html)] [[arxiv](https://arxiv.org/abs/1908.04355)] [[paper with code](https://paperswithcode.com/paper/adversarial-neural-pruning)] [[code](https://github.com/divyam3897/ANP_VS)]

- Proving the Lottery Ticket Hypothesis: Pruning is All You Need [[paper](https://proceedings.mlr.press/v119/malach20a.html)] [[arxiv](https://arxiv.org/abs/2002.00585)] [[paper with code](https://paperswithcode.com/paper/proving-the-lottery-ticket-hypothesis-pruning)]

- DropNet: Reducing Neural Network Complexity via Iterative Pruning [[paper](https://proceedings.mlr.press/v119/tan20a.html)] [[arxiv](https://arxiv.org/abs/2207.06646)] [[paper with code](https://paperswithcode.com/paper/dropnet-reducing-neural-network-complexity-1)] [[code](https://github.com/tanchongmin/DropNet)]

- Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection [[paper](https://proceedings.mlr.press/v119/ye20b.html)] [[arxiv](https://arxiv.org/abs/2003.01794)] [[paper with code](https://paperswithcode.com/paper/good-subnetworks-provably-exist-pruning-via)] [[code](https://github.com/lushleaf/Network-Pruning-Greedy-Forward-Selection)]


## IJCAI-2020


- Channel Pruning via Automatic Structure Search [[paper](https://www.ijcai.org/proceedings/2020/94)] [[arxiv](https://arxiv.org/abs/2001.08565)] [[paper with code](https://paperswithcode.com/paper/channel-pruning-via-automatic-structure)] [[code](https://github.com/lmbxmu/ABCPruner)]

- TRP: Trained Rank Pruning for Efficient Deep Neural Networks [[paper](https://www.ijcai.org/proceedings/2020/136)] [[arxiv](https://arxiv.org/abs/2004.14566)] [[paper with code](https://paperswithcode.com/paper/trp-trained-rank-pruning-for-efficient-deep)] [[code](https://github.com/yuhuixu1993/Trained-Rank-Pruning)]

- Beyond Network Pruning: a Joint Search-and-Training Approach [[paper](https://www.ijcai.org/proceedings/2020/358)]

- Feature Statistics Guided Efficient Filter Pruning [[paper](https://www.ijcai.org/proceedings/2020/363)] [[arxiv](https://arxiv.org/abs/2005.12193)] [[paper with code](https://paperswithcode.com/paper/feature-statistics-guided-efficient-filter)]

- Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis and its Generalization Error [[paper](https://www.ijcai.org/proceedings/2020/393)] [[arxiv](https://arxiv.org/abs/1808.08558)] [[paper with code](https://paperswithcode.com/paper/spectral-pruning-compressing-deep-neural)]

- Towards Real-Time DNN Inference on Mobile Platforms with Model Pruning and Compiler Optimization [[paper](https://www.ijcai.org/proceedings/2020/778)] [[arxiv](https://arxiv.org/abs/2004.11250)] [[paper with code](https://paperswithcode.com/paper/towards-real-time-dnn-inference-on-mobile)]


## AAAI-2020


- AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/5924)] [[arxiv](https://arxiv.org/abs/1907.03141)] [[paper with code](https://paperswithcode.com/paper/autoslim-an-automatic-dnn-structured-pruning)]

- Layerwise Sparse Coding for Pruned Deep Neural Networks with Extreme Compression Ratio [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/5927)]

- PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for Real-Time Execution on Mobile Devices [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/5954)] [[arxiv](https://arxiv.org/abs/1909.05073)] [[paper with code](https://paperswithcode.com/paper/pconv-the-missing-but-desirable-sparsity-in)]

- Learning Agent Communication under Limited Bandwidth by Message Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/5957)] [[arxiv](https://arxiv.org/abs/1912.05304)] [[paper with code](https://paperswithcode.com/paper/learning-agent-communication-under-limited)]

- DARB: A Density-Adaptive Regular-Block Pruning for Deep Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6000)]

- Reborn Filters: Pruning Convolutional Neural Networks with Limited Data [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6058)]

- Dynamic Network Pruning with Interpretable Layerwise Channel Selection [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6098)]

- Harmonious Coexistence of Structured Weight Pruning and Ternarization for Deep Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6138)]

- Lifted Fact-Alternating Mutex Groups and Pruned Grounding of Classical Planning Problems [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6536)]

- Novel Is Not Always Better: On the Relation between Novelty and Dominance Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6541)]

- Channel Pruning Guided by Classification Loss and Feature Importance [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6720)] [[arxiv](https://arxiv.org/abs/2003.06757)] [[paper with code](https://paperswithcode.com/paper/channel-pruning-guided-by-classification-loss)]

- Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6779)] [[arxiv](https://arxiv.org/abs/1911.11170)] [[paper with code](https://paperswithcode.com/paper/real-time-object-tracking-via-meta-learning)]

- Pruning from Scratch [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6910)] [[arxiv](https://arxiv.org/abs/1909.12579)] [[paper with code](https://paperswithcode.com/paper/pruning-from-scratch)] [[code](https://github.com/zheng-ningxin/Pruning-from-scratch)]



# 2019


## NeurIPS-2019


- AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters [[paper](https://proceedings.neurips.cc/paper_files/paper/2019/hash/4efc9e02abdab6b6166251918570a307-Abstract.html)] [[paper with code](https://paperswithcode.com/paper/autoprune-automatic-network-pruning-by)] [[code](https://github.com/xxshdw/auto_prune)]

- Network Pruning via Transformable Architecture Search [[paper](https://proceedings.neurips.cc/paper_files/paper/2019/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html)] [[arxiv](https://arxiv.org/abs/1905.09717)] [[paper with code](https://paperswithcode.com/paper/network-pruning-via-transformable)] [[code](https://github.com/D-X-Y/NAS-Projects)]

- Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2019/hash/b51a15f382ac914391a58850ab343b00-Abstract.html)] [[arxiv](https://arxiv.org/abs/1909.08174)] [[paper with code](https://paperswithcode.com/paper/gate-decorator-global-filter-pruning-method)] [[code](https://github.com/youzhonghui/gate-decorator-pruning)]

- Global Sparse Momentum SGD for Pruning Very Deep Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f34185c4ca5d58e781d4f14173d41e5d-Abstract.html)] [[arxiv](https://arxiv.org/abs/1909.12778)] [[paper with code](https://paperswithcode.com/paper/global-sparse-momentum-sgd-for-pruning-very)] [[code](https://github.com/DingXiaoH/GSM-SGD)]


## CVPR-2019


- Variational Convolutional Neural Network Pruning [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Variational_Convolutional_Neural_Network_Pruning_CVPR_2019_paper.html)] [[paper with code](https://paperswithcode.com/paper/variational-convolutional-neural-network)]

- Towards Optimal Structured CNN Pruning via Generative Adversarial Learning [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Towards_Optimal_Structured_CNN_Pruning_via_Generative_Adversarial_Learning_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1903.09291)] [[paper with code](https://paperswithcode.com/paper/towards-optimal-structured-cnn-pruning-via)] [[code](https://github.com/ShaohuiLin/GAL)]

- Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1811.00250)] [[paper with code](https://paperswithcode.com/paper/pruning-filter-via-geometric-median-for-deep)] [[code](https://github.com/he-y/filter-pruning-geometric-median)]

- Centripetal SGD for Pruning Very Deep Convolutional Networks With Complicated Structure [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1904.03837)] [[paper with code](https://paperswithcode.com/paper/centripetal-sgd-for-pruning-very-deep)] [[code](https://github.com/ShawnDing1994/Centripetal-SGD)]

- Structured Pruning of Neural Networks With Budget-Aware Regularization [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Lemaire_Structured_Pruning_of_Neural_Networks_With_Budget-Aware_Regularization_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1811.09332)] [[paper with code](https://paperswithcode.com/paper/structured-pruning-of-neural-networks-with)]

- Partial Order Pruning: For Best Speed/Accuracy Trade-Off in Neural Architecture Search [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Partial_Order_Pruning_For_Best_SpeedAccuracy_Trade-Off_in_Neural_Architecture_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1903.03777)] [[paper with code](https://paperswithcode.com/paper/partial-order-pruning-for-best-speedaccuracy)] [[code](https://github.com/lixincn2015/Partial-Order-Pruning)]

- Importance Estimation for Neural Network Pruning [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1906.10771)] [[paper with code](https://paperswithcode.com/paper/importance-estimation-for-neural-network-1)] [[code](https://github.com/NVlabs/Taylor_pruning)]


## ICLR-2019


- Dynamic Channel Pruning: Feature Boosting and Suppression [[paper](https://openreview.net/forum?id=BJxh2j0qYm)] [[arxiv](https://arxiv.org/abs/1810.05331)] [[paper with code](https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and)] [[code](https://github.com/deep-fry/mayo)] [[openreview](https://openreview.net/forum?id=BJxh2j0qYm)]

- Rethinking the Value of Network Pruning [[paper](https://openreview.net/forum?id=rJlnB3C5Ym)] [[arxiv](https://arxiv.org/abs/1810.05270)] [[paper with code](https://paperswithcode.com/paper/rethinking-the-value-of-network-pruning)] [[code](https://github.com/Eric-mingjie/rethinking-network-pruning)] [[openreview](https://openreview.net/forum?id=rJlnB3C5Ym)]

- SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY [[paper](https://openreview.net/forum?id=B1VZqjAcYX)] [[arxiv](https://arxiv.org/abs/1810.02340)] [[paper with code](https://paperswithcode.com/paper/snip-single-shot-network-pruning-based-on)] [[code](https://github.com/namhoonlee/snip-public)] [[openreview](https://openreview.net/forum?id=B1VZqjAcYX)]


## ICCV-2019


- MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1903.10258)] [[paper with code](https://paperswithcode.com/paper/metapruning-meta-learning-for-automatic)] [[code](https://github.com/liuzechun/MetaPruning)]

- Accelerate CNN via Recursive Bayesian Pruning [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1812.00353)] [[paper with code](https://paperswithcode.com/paper/network-compression-via-recursive-bayesian)]

- DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Duggal_DeepPruner_Learning_Efficient_Stereo_Matching_via_Differentiable_PatchMatch_ICCV_2019_paper.html)] [[arxiv](https://arxiv.org/abs/1909.05845)] [[paper with code](https://paperswithcode.com/paper/deeppruner-learning-efficient-stereo-matching)] [[code](https://github.com/uber-research/DeepPruner)]


## ICML-2019


- Approximated Oracle Filter Pruning for Destructive CNN Width Optimization [[paper](https://proceedings.mlr.press/v97/ding19a.html)] [[arxiv](https://arxiv.org/abs/1905.04748)] [[paper with code](https://paperswithcode.com/paper/approximated-oracle-filter-pruning-for)] [[code](https://github.com/ShawnDing1994/AOFP)]

- Collaborative Channel Pruning for Deep Networks [[paper](https://proceedings.mlr.press/v97/peng19c.html)]

- EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis [[paper](https://proceedings.mlr.press/v97/wang19g.html)] [[arxiv](https://arxiv.org/abs/1905.05934)] [[paper with code](https://paperswithcode.com/paper/eigendamage-structured-pruning-in-the)] [[code](https://github.com/alecwangcq/EigenDamage-Pytorch)]


## ACL-2019


- Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [[paper](https://aclanthology.org/P19-1580/)] [[arxiv](https://arxiv.org/abs/1905.09418)] [[paper with code](https://paperswithcode.com/paper/analyzing-multi-head-self-attention)] [[code](https://github.com/lena-voita/the-story-of-heads)]


## IJCAI-2019


- Cooperative Pruning in Cross-Domain Deep Neural Network Compression [[paper](https://www.ijcai.org/proceedings/2019/291)]

- Play and Prune: Adaptive Filter Pruning for Deep Model Compression [[paper](https://www.ijcai.org/proceedings/2019/480)] [[arxiv](https://arxiv.org/abs/1905.04446)] [[paper with code](https://paperswithcode.com/paper/play-and-prune-adaptive-filter-pruning-for)] [[code](https://github.com/softsys4ai/neural-distiller)]

- COP: Customized Deep Model Compression via Regularized Correlation-Based Filter-Level Pruning [[paper](https://www.ijcai.org/proceedings/2019/525)] [[arxiv](https://arxiv.org/abs/1906.10337)] [[paper with code](https://paperswithcode.com/paper/cop-customized-deep-model-compression-via)] [[code](https://github.com/ZJULearning/COP)]


## AAAI-2019


- Separator-Based Pruned Dynamic Programming for Steiner Tree [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/3965)]

- A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/4209)] [[arxiv](https://arxiv.org/abs/1812.06611)] [[paper with code](https://paperswithcode.com/paper/a-layer-decomposition-recomposition-framework)]

- Efficient Data Point Pruning for One-Class SVM [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/4239)]


# 2018


## NeurIPS-2018


- Discrimination-aware Channel Pruning for Deep Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2018/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html)] [[arxiv](https://arxiv.org/abs/1810.11809)] [[paper with code](https://paperswithcode.com/paper/discrimination-aware-channel-pruning-for-deep)] [[code](https://github.com/SCUT-AILab/DCP)]

- An Efficient Pruning Algorithm for Robust Isotonic Regression [[paper](https://proceedings.neurips.cc/paper_files/paper/2018/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html)] [[paper with code](https://paperswithcode.com/paper/an-efficient-pruning-algorithm-for-robust)]

- Frequency-Domain Dynamic Pruning for Convolutional Neural Networks [[paper](https://proceedings.neurips.cc/paper_files/paper/2018/hash/a9a6653e48976138166de32772b1bf40-Abstract.html)] [[paper with code](https://paperswithcode.com/paper/frequency-domain-dynamic-pruning-for)]


## CVPR-2018


- BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning [[paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_BPGrad_Towards_Global_CVPR_2018_paper.html)] [[arxiv](https://arxiv.org/abs/1711.06959)] [[paper with code](https://paperswithcode.com/paper/bpgrad-towards-global-optimality-in-deep)]

- PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning [[paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html)] [[arxiv](https://arxiv.org/abs/1711.05769)] [[paper with code](https://paperswithcode.com/paper/packnet-adding-multiple-tasks-to-a-single)] [[code](https://github.com/arunmallya/packnet)]

- CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization [[paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.html)] [[paper with code](https://paperswithcode.com/paper/clip-q-deep-network-compression-learning-by)]

- âLearning-Compressionâ Algorithms for Neural Net Pruning [[paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.html)]

- NISP: Pruning Networks Using Neuron Importance Score Propagation [[paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_NISP_Pruning_Networks_CVPR_2018_paper.html)] [[arxiv](https://arxiv.org/abs/1711.05908)] [[paper with code](https://paperswithcode.com/paper/nisp-pruning-networks-using-neuron-importance)]


## ICLR-2018


- Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers [[paper](https://openreview.net/forum?id=HJ94fqApW)] [[arxiv](https://arxiv.org/abs/1802.00124)] [[paper with code](https://paperswithcode.com/paper/rethinking-the-smaller-norm-less-informative)] [[code](https://github.com/bobye/batchnorm_prune)] [[openreview](https://openreview.net/forum?id=HJ94fqApW)]

- Stochastic Activation Pruning for Robust Adversarial Defense [[paper](https://openreview.net/forum?id=H1uR4GZRZ)] [[arxiv](https://arxiv.org/abs/1803.01442)] [[paper with code](https://paperswithcode.com/paper/stochastic-activation-pruning-for-robust)] [[code](https://github.com/Guneet-Dhillon/Stochastic-Activation-Pruning)] [[openreview](https://openreview.net/forum?id=H1uR4GZRZ)]

- Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio [[paper](https://openreview.net/forum?id=S1D8MPxA-)] [[paper with code](https://paperswithcode.com/paper/viterbi-based-pruning-for-sparse-matrix-with)] [[openreview](https://openreview.net/forum?id=S1D8MPxA-)]


## ECCV-2018


- A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [[paper](https://www.ecva.net/papers/eccv_2018/papers_ECCV/html/Tianyun_Zhang_A_Systematic_DNN_ECCV_2018_paper.php)] [[arxiv](https://arxiv.org/abs/1804.03294)] [[paper with code](https://paperswithcode.com/paper/a-systematic-dnn-weight-pruning-framework)] [[code](https://github.com/KaiqiZhang/admm-pruning)]


## IJCAI-2018


- Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks [[paper](https://www.ijcai.org/proceedings/2018/309)] [[arxiv](https://arxiv.org/abs/1808.06866)] [[paper with code](https://paperswithcode.com/paper/soft-filter-pruning-for-accelerating-deep)] [[code](https://github.com/he-y/soft-filter-pruning)]

- Efficient DNN Neuron Pruning by Minimizing Layer-wise Nonlinear Reconstruction Error [[paper](https://www.ijcai.org/proceedings/2018/318)]

- Optimization based Layer-wise Magnitude-based Pruning for DNN Compression [[paper](https://www.ijcai.org/proceedings/2018/330)]

- Accelerating Convolutional Networks via Global & Dynamic Filter Pruning [[paper](https://www.ijcai.org/proceedings/2018/336)]

- Where to Prune: Using LSTM to Guide End-to-end Pruning [[paper](https://www.ijcai.org/proceedings/2018/445)]

- Efficient Pruning of Large Knowledge Graphs [[paper](https://www.ijcai.org/proceedings/2018/564)]


## AAAI-2018


- Auto-Balanced Filter Pruning for Efficient Convolutional Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/12262)]